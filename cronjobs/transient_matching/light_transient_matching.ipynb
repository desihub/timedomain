{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install --user alerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import astropy.config\n",
    "astropy.config.get_cache_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord, match_coordinates_sky, Angle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "global db_filename\n",
    "db_filename = '/global/cfs/cdirs/desi/science/td/daily-search/transients_search.db'\n",
    "global exposure_path\n",
    "exposure_path = os.environ[\"DESI_SPECTRO_REDUX\"]\n",
    "global color_band\n",
    "color_band = \"r\"\n",
    "#import warnings\n",
    "#from ALeRCE_ledgermaker import access_alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the file names\n",
    "def all_candidate_filenames(transient_dir: str):\n",
    "    \n",
    "    # This function grabs the names of all input files in the transient directory and does some python string manipulation\n",
    "    # to grab the names of the input files with full path and the filenames themselves.\n",
    "\n",
    "    #transient_dir = \n",
    "    try:\n",
    "        filenames_read = glob.glob(transient_dir + \"/*.fits\") # Hardcoding is hopefully a temporary measure.\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not grab/find any fits in the transient spectra directory:\")\n",
    "        print(transient_dir)\n",
    "        filenames_read = [] # Just in case\n",
    "        #filenames_out = [] # Just in case\n",
    "        raise SystemExit(\"Exitting.\")\n",
    "        \n",
    "    #else:\n",
    "        #filenames_out = [s.split(\".\")[0] for s in filenames_read]\n",
    "        #filenames_out = [s.split(\"/\")[-1] for s in filenames_read]\n",
    "        #filenames_out = [s.replace(\"in\", \"out\") for s in filenames_out]\n",
    "        \n",
    "    return filenames_read #, filenames_out\n",
    "\n",
    "#path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "#print(all_candidate_filenames(path_to_transient)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ALeRCE_ledgermaker https://github.com/alercebroker/alerce_client\n",
    "import requests\n",
    "#import matplotlib as mpl\n",
    "#import matplotlib.pyplot as plt\n",
    "from astropy.time import Time\n",
    "#import pandas as pd\n",
    "from alerce.core import Alerce\n",
    "from alerce.exceptions import APIError\n",
    "\n",
    "alerce_client = Alerce()\n",
    "\n",
    "def access_alerts(lastmjd=None, classifier='stamp_classifier', class_names=['SN', 'AGN']):\n",
    "    if type(class_names) is not list:\n",
    "        raise TypeError('Argument `class_names` must be a list.')\n",
    "        \n",
    "    dataframes = []\n",
    "    for class_name in class_names:\n",
    "        data = alerce_client.query_objects(classifier=classifier,\n",
    "                                           class_name=class_name, \n",
    "                                           order_by='oid',\n",
    "                                           order_mode='ASC',\n",
    "                                           page_size=1000,\n",
    "                                           format='pandas')\n",
    "        \n",
    "        if lastmjd is not None:\n",
    "            select = data['lastmjd'] >= lastmjd\n",
    "            data = data[select]\n",
    "            \n",
    "        dataframes.append(data)\n",
    "    \n",
    "    return pd.concat(dataframes).sort_values(by='lastmjd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/desihub/timedomain/blob/master/too_ledgers/decam_TAMU_ledgermaker.ipynb\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "def access_decam_data(url, overwrite=False):\n",
    "    \"\"\"Download reduced DECam transient data from Texas A&M.\n",
    "    Cache the data to avoid lengthy and expensive downloads.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL for accessing the data.\n",
    "    overwrite : bool\n",
    "        Download new data and overwrite the cached data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    decam_transients : pandas.DataFrame\n",
    "        Table of transient data.\n",
    "    \"\"\"\n",
    "    folders = url.split('/')\n",
    "    thedate = folders[-1] if len(folders[-1]) > 0 else folders[-2]\n",
    "    outfile = '{}.csv'.format(thedate)\n",
    "    \n",
    "    if os.path.exists(outfile) and not overwrite:\n",
    "        # Access cached data.\n",
    "        decam_transients = pd.read_csv(outfile)\n",
    "    else:\n",
    "        # Download the DECam data index.\n",
    "        # A try/except is needed because the datahub SSL certificate isn't playing well with URL requests.\n",
    "        try:\n",
    "            decam_dets = requests.get(url).text\n",
    "        except:\n",
    "            requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)\n",
    "            decam_dets = requests.get(url, verify=False).text\n",
    "            \n",
    "        # Convert transient index page into scrapable data using BeautifulSoup.\n",
    "        soup = BeautifulSoup(decam_dets)\n",
    "        \n",
    "        # Loop through transient object summary JSON files indexed in the main transient page.\n",
    "        # Download the JSONs and dump the info into a Pandas table.\n",
    "        decam_transients = None\n",
    "        j = 0\n",
    "\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            if 'object-summary.json' in a:\n",
    "                link = a['href'].replace('./', '')\n",
    "                summary_url  = url + link        \n",
    "                summary_text = requests.get(summary_url, verify=False).text\n",
    "                summary_data = json.loads(summary_text)\n",
    "\n",
    "                j += 1\n",
    "                #print('Accessing {:3d}  {}'.format(j, summary_url)) # Modified by Matt\n",
    "\n",
    "                if decam_transients is None:\n",
    "                    decam_transients = pd.DataFrame(summary_data, index=[0])\n",
    "                else:\n",
    "                    decam_transients = pd.concat([decam_transients, pd.DataFrame(summary_data, index=[0])])\n",
    "                    \n",
    "        # Cache the data for future access.\n",
    "        print('Saving output to {}'.format(outfile))\n",
    "        decam_transients.to_csv(outfile, index=False)\n",
    "        \n",
    "    return decam_transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fits_ra_dec(filepath: str, transient_candidate = True):\n",
    "    \n",
    "    if transient_candidate:\n",
    "        hdu_num = 1\n",
    "    else:\n",
    "        hdu_num = 5\n",
    "    \n",
    "    try:\n",
    "        with fits.open(filepath) as hdu1:\n",
    "    \n",
    "            data_table = Table(hdu1[hdu_num].data) #columns\n",
    "        \n",
    "            #targ_id = data_table['TARGETID']\n",
    "            targ_ra = data_table['TARGET_RA'].data # Now it's a numpy array\n",
    "            targ_dec = data_table['TARGET_DEC'].data\n",
    "            #targ_mjd = data_table['MJD'][0] some have different versions of this so this is a *bad* idea... at least now I know the try except works!\n",
    "            \n",
    "            if not transient_candidate:\n",
    "                targ_mjd = hdu1[hdu_num].header['MJD-OBS']\n",
    "            \n",
    "    except:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        print(\"Could not open or use:\", filename)\n",
    "        print(\"In path:\", filepath)\n",
    "        print(\"Trying the next file...\")\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    if transient_candidate:\n",
    "        targ_mjd = filepath.split(\"/\")[-1].split(\"_\")[-2] #to grab the date\n",
    "        targ_mjd = targ_mjd[:4]+\"-\"+targ_mjd[4:6]+\"-\"+targ_mjd[6:] # Adding dashes for Time\n",
    "        targ_mjd = Time(targ_mjd).mjd\n",
    "    \n",
    "    return targ_ra, targ_dec, targ_mjd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(path_in: str, max_sep: float, transient_cand = True, target_ra_dec_date = ()): #, #first_run: bool = True): # To be cleaned up...\n",
    "    \n",
    "    max_sep *= u.arcsec\n",
    "    \n",
    "    if not target_ra_dec_date:\n",
    "        target_ras, target_decs, obs_mjd = read_fits_ra_dec(path_in, transient_cand)\n",
    "    else:\n",
    "        target_ras, target_decs, obs_mjd = target_ra_dec_date\n",
    "        \n",
    "    if not target_ras.size:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    nan_ra = np.isnan(target_ras)\n",
    "    nan_dec = np.isnan(target_decs)\n",
    "    \n",
    "    if np.any(nan_ra) or np.any(nan_dec):\n",
    "        print(\"NaNs found, removing them from array (not FITS) before match.\")\n",
    "        #print(\"Original length (ra, dec): \", len(target_ras), len(target_decs))\n",
    "        nans = np.logical_not(np.logical_and(nan_ra, nan_dec))\n",
    "        target_ras = target_ras[nans] # Logic masking, probably more efficient\n",
    "        target_decs = target_decs[nans]\n",
    "        #print(\"Reduced length (ra, dec):\", len(target_ras), len(target_decs))\n",
    "        #print(np.where(np.isnan(target_ras) == True))\n",
    "        #print(target_ras[:100])\n",
    "        #print(np.where(np.isnan(target_decs) == True))\n",
    "        #target_ras = np.delete(target_ras, nans)\n",
    "        #target_decs = np.delete(target_decs, nans)\n",
    "    \n",
    "    alerts = access_alerts(lastmjd=obs_mjd-28) # Modified Julian Day #.mjd\n",
    "    # Write function to decide if alerce or DECAM\n",
    "    tree_name = \"kdtree_\" + str(obs_mjd - 28)\n",
    "    \n",
    "    # For each fits file, look at one month before the observation from Alerce\n",
    "    alerts_ra = alerts['meanra'].to_numpy()\n",
    "    alerts_dec = alerts['meandec'].to_numpy()\n",
    "\n",
    "    coo_trans_search = SkyCoord(target_ras*u.deg, target_decs*u.deg)\n",
    "    #print(coo_trans_search)\n",
    "    coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "\n",
    "    #if first_run:\n",
    "    idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts, storekdtree = tree_name) # store tree to speed up subsequent results\n",
    "    #else:\n",
    "    #    idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts) # I'm not 100% sure but I don't want to make it think it has to overwrite every time and thus take more time\n",
    "\n",
    "    sep_constraint = d2d_trans < max_sep\n",
    "    trans_matches = coo_trans_search[sep_constraint]\n",
    "    alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "    \n",
    "    if trans_matches.size:\n",
    "        sort_dist = np.sort(d2d_trans)\n",
    "        print(\"Minimum distance found: \", sort_dist[0])\n",
    "        print(\"5 closest (in case there's more than one): \", sort_dist[:5])\n",
    "        print()\n",
    "\n",
    "    #if trans_matches.size:\n",
    "        #all_trans_matches.append(trans_matches)\n",
    "        #all_alerts_matches.append(alerts_matches)\n",
    "\n",
    "    return trans_matches, alerts_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def matching(path_in: str, max_sep: float):\n",
    "    \n",
    "    max_sep *= u.arcsec\n",
    "    \n",
    "    target_ras, target_decs, obs_date = read_fits_ra_dec(path_in)\n",
    "    \n",
    "    filename = path_in.split(\"/\")[-1]\n",
    "    tree_name = \"kdtree_\" + filename[:-5] # Gets rid of the .fits at the end\n",
    "        \n",
    "    if not target_ras.size:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    alerts = access_alerts(lastmjd=obs_date-28) # Modified Julian Day #.mjd\n",
    "    # For each fits file, look at one month before the observation from Alerce\n",
    "    alerts_ra = alerts['meanra'].to_numpy()\n",
    "    alerts_dec = alerts['meandec'].to_numpy()\n",
    "\n",
    "    coo_trans_search = SkyCoord(target_ras*u.deg, target_decs*u.deg)\n",
    "    #print(coo_trans_search)\n",
    "    coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "    \n",
    "    print(tree_name)\n",
    "    idx_trans, d2d_alerts, d3d_alerts = match_coordinates_sky(coo_alerts, coo_trans_search, storekdtree = tree_name) # Store tree to speed up subsequent results\n",
    "\n",
    "    sep_constraint = d2d_alerts < max_sep\n",
    "    alerts_matches = coo_alerts[sep_constraint]\n",
    "    trans_matches = coo_trans_search[idx_trans[sep_constraint]]\n",
    "\n",
    "    #if trans_matches.size:\n",
    "        #all_trans_matches.append(trans_matches)\n",
    "        #all_alerts_matches.append(alerts_matches)\n",
    "\n",
    "    return trans_matches, alerts_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def check_corners(fits_path, ):\n",
    "\n",
    "    paths_to_fits = all_candidate_filenames(fits_path)\n",
    "    target_ra, target_dec, obs_date = read_fits_ra_dec(paths_to_fits)\n",
    "\n",
    "    min_ra_idx = np.argmin(target_ra) # If we use data_table['TARGET_RA'].data it'll convert the data into a numpy array\n",
    "    max_ra_idx = np.argmax(target_ra)\n",
    "    min_dec_idx = np.argmin(target_dec)\n",
    "    max_dec_idx = np.argmax(target_dec) # What if it's inside the area and the area is larger than 5\"? If/then statement!\n",
    "\n",
    "    min_max_ra = [target_ra[min_ra_idx], target_ra[max_ra_idx], target_ra[min_dec_idx], target_ra[max_dec_idx]]\n",
    "    min_max_dec = [target_dec[min_ra_idx], target_dec[max_ra_idx], target_dec[min_dec_idx], target_dec[max_dec_idx]]\n",
    "\n",
    "    min_max = SkyCoord(min_max_ra*u.deg, min_max_dec*u.deg)\n",
    "\n",
    "    idx, sep2d, d3d = match_coordinates_sky(min_max, min_max, 2)\n",
    "\n",
    "    max_dist = max(sep2d)\n",
    "    # Optimal - combine all frames into one giant numpy array and check the four corners of those - then do this on the alerts, if you find nothing, good, move on\n",
    "    # If you do find something, figure out which one its closest to by the index and do that\n",
    "    #print(d3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Reading FITS and grabbing RA and DEC\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #alerts_coords = np.array(zip(alerts['meanra'].to_numpy(), alerts['meandec'].to_numpy()))\n",
    "    \n",
    "    path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "    paths_to_fits = all_candidate_filenames(path_to_transient)\n",
    "    #fits_image_filename = fits.util.get_testdata_filepath(fits_name[0])\n",
    "    \n",
    "    # If done in serial, would it be faster to chunk all of the ra's and dec's together before comparing? Actually maybe not because of kdtree.\n",
    "    # If done in parallel though, I doubt it.\n",
    "    \n",
    "#     max_sep = 5.0 * u.arcsec\n",
    "    \n",
    "    all_trans_matches = []\n",
    "    all_alerts_matches = []\n",
    "    \n",
    "    #trans_match, alerts_match = matching(paths_to_fits[0], 5.0) # To get the kdtree started *shrugs*\n",
    "    #if trans_match.size:\n",
    "    #    all_trans_matches.append(trans_match)\n",
    "    #    all_alerts_matches.append(alerts_match)\n",
    "        \n",
    "    for path in paths_to_fits[:2]:\n",
    "        trans_matches, alerts_matches = matching(path, 5.0)\n",
    "#         target_ras, target_decs, target_ids = read_fits_ra_dec(path)\n",
    "        \n",
    "#         if not target_ras.size:\n",
    "#             continue\n",
    "\n",
    "#         coo_trans_search = SkyCoord(targ_ra*u.deg, targ_dec*u.deg)\n",
    "#         coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "\n",
    "#         idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts, storekdtree= True) #'kdtree_alerts') # store tree to speed up subsequent results\n",
    "\n",
    "#         sep_constraint = d2d_trans < max_sep\n",
    "#         trans_matches = coo_trans_search[sep_constraint]\n",
    "#         alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "        #print(trans_matches)\n",
    "        if trans_matches.size:\n",
    "            all_trans_matches.append(trans_matches)\n",
    "            all_alerts_matches.append(alerts_matches)\n",
    "        \n",
    "    print(all_trans_matches)\n",
    "\n",
    "    #print(sorted(set(idx_trans)))\n",
    "    #print(d2d_trans.arcsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_frames(exp_d: str):\n",
    "    # Grabbing the frame fits files\n",
    "    \n",
    "    # This function grabs the names of all input files in the transient directory and does some python string manipulation\n",
    "    # to grab the names of the input files with full path and the filenames themselves.\n",
    "\n",
    "    try:\n",
    "        filenames_read = glob.glob(exp_d + \"/cframe-\" + color_band + \"*.fits\") # Only need one of b, r, z\n",
    "        # sframes not flux calibrated\n",
    "        # May want to use tiles... coadd (will need later, but not now)\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not grab/find any fits in the exposure directory:\")\n",
    "        print(exp_d)\n",
    "        filenames_read = [] # Just in case\n",
    "        #filenames_out = [] # Just in case\n",
    "        raise SystemExit(\"Exitting.\")\n",
    "        \n",
    "    #else:\n",
    "        #filenames_out = [s.split(\".\")[0] for s in filenames_read]\n",
    "        #filenames_out = [s.split(\"/\")[-1] for s in filenames_read]\n",
    "        #filenames_out = [s.replace(\"in\", \"out\") for s in filenames_out]\n",
    "        \n",
    "    return filenames_read #, filenames_out\n",
    "\n",
    "#path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "#print(all_candidate_filenames(path_to_transient)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_matches_to_file(start_date, end_date, all_matches_dict, ledger_type):\n",
    "    filename = \"./matches_\" + ledger_type + \"_\" + start_date + \"-\" + end_date + \".txt\"\n",
    "    with open(filename, 'w') as mfile:\n",
    "        mfile.write(\"date:\\n\")\n",
    "        mfile.write(\"\\tframename; (RA, DEC); Table Index in FIBERMAP; Alert (RA, DEC)\\n\")\n",
    "        for key, val in all_matches_dict.items():\n",
    "            if val:\n",
    "                mfile.write(str(key) + \": \\n\")\n",
    "                for vals in val:\n",
    "                    mfile.write(\"\\t\")\n",
    "                    #print(\", \".join(vals))\n",
    "                    mfile.write(\"; \".join(str(x) for x in vals))\n",
    "                    mfile.write(\"\\n\")\n",
    "        #for match in exp_matches:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_check(ledger_df = None, ledger_type = ''):\n",
    "\n",
    "    #for obsdate,tile_number in obsdates_tilenumbers:\n",
    "        #redux = '/'.join([os.environ['DESI_SPECTRO_REDUX'], args.redux, 'tiles'])\n",
    "        #prefix_in = '/'.join([redux, tile_number, obsdate])\n",
    "\n",
    "    # previous_date_ranges: \"20201130\" - \"20210302\"\n",
    "    query_date_start = \"20201130\" #\"20210228\"\n",
    "    query_date_end = \"20210505\" #\"20210501\"\n",
    "\n",
    "    query2 = \"PRAGMA table_info(exposures)\"\n",
    "    query3 = \"PRAGMA table_info(tiles)\"\n",
    "    # Crossmatch across tiles and exposures to grab obsdate via tileid\n",
    "    query_match = \"SELECT distinct tilera, tiledec, obsdate, obsmjd, expid, exposures.tileid from exposures INNER JOIN tiles ON exposures.tileid = tiles.tileid where obsdate BETWEEN \" + \\\n",
    "        query_date_start + \" AND \" + query_date_end + \";\" #obsdate>20210228 \n",
    "    #query_dates = \"SELECT obsdate from exposures INNER JOIN tiles ON exposures.tileid = tiles.tileid where obsdate BETWEEN \" + \\\n",
    "        #query_date_start + \" AND \" + query_date_end + \";\"\n",
    "\n",
    "    #cur.execute(query2)\n",
    "    #row2 = cur.fetchall()\n",
    "    #for i in row2:\n",
    "    #    print(i[:])\n",
    "\n",
    "    conn = sqlite3.connect(db_filename)\n",
    "\n",
    "    conn.row_factory = sqlite3.Row # https://docs.python.org/3/library/sqlite3.html#sqlite3.Row\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    #cur.execute(query)\n",
    "    #rows = cur.fetchall()\n",
    "\n",
    "    cur.execute(query_match)\n",
    "    matches_list = cur.fetchall()\n",
    "    cur.close()\n",
    "\n",
    "    # I knew there was a way! THANK YOU!\n",
    "    # https://stackoverflow.com/questions/11276473/append-to-a-dict-of-lists-with-a-dict-comprehension\n",
    "    date_dict = {k['obsdate'] : list(filter(lambda x:x['obsdate'] == k['obsdate'], matches_list)) for k in matches_list}\n",
    "\n",
    "    #cur.execute(query_dates)\n",
    "    #dates = set(cur.fetchall())\n",
    "\n",
    "    alert_matches_dict = {} #{i['obsdate']: [] for i in matches_list}\n",
    "\n",
    "    all_trans_matches = []\n",
    "    all_alerts_matches = []\n",
    "    \n",
    "    #decam_transients = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "    if ledger_type.upper() == 'DECAM_TAMU':\n",
    "        if ledger_df.empty:\n",
    "            ledger_df = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "\n",
    "    for date, row in date_dict.items():\n",
    "        \n",
    "        date_str = str(date)\n",
    "        date_str = date_str[:4]+\"-\"+date_str[4:6]+\"-\"+date_str[6:] # Adding dashes for Time\n",
    "        obs_mjd = Time(date_str).mjd\n",
    "\n",
    "        # This method is *technically* safer than doing a double list comprehension with set albeit slower\n",
    "        # The lists are small enough that speed shouldn't matter here\n",
    "        unique_ra_dec = set([(i[0], i[1]) for i in row]) # there's probably a way to do this in SQL... oh well\n",
    "        #print(unique_ra_dec)\n",
    "        target_ras, target_decs = zip(*unique_ra_dec)\n",
    "        \n",
    "        if ledger_type.upper() == 'ALERCE':\n",
    "            if ledger_df.empty:\n",
    "                ledger_df = access_alerts(lastmjd=obs_mjd-28) # Modified Julian Day #.mjd\n",
    "        elif ledger_type.upper() == 'DECAM_TAMU':\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Cannot use alerts broker/ledger provided. Stopping before match.\")\n",
    "            return {}\n",
    "        \n",
    "        #print(target_ras)\n",
    "        trans_matches, alert_matches = decam_matching(target_ras, target_decs, obs_mjd, '', max_sep = 1.8, sep_units = 'deg', ledger_df_in = ledger_df, ledger_type_in = ledger_type)\n",
    "        \n",
    "        if trans_matches.size:\n",
    "            #print(len(row))\n",
    "            all_trans_matches.append(trans_matches)\n",
    "            # To retrieve from SkyCoord into numpy float, x.ra.deg, x.dec.deg\n",
    "            all_alerts_matches.append(alert_matches)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        alert_matches_dict[date] = []\n",
    "\n",
    "        for tup in trans_matches:\n",
    "            ra = tup.ra.deg\n",
    "            dec = tup.dec.deg\n",
    "            match_rows = [i for i in row if (i['tilera'], i['tiledec']) == (ra, dec)]\n",
    "            alert_matches_dict[date].extend(match_rows)\n",
    "            \n",
    "    return alert_matches_dict\n",
    "            \n",
    "            \n",
    "#print(alert_matches_dict)\n",
    "        \n",
    "# I bet if we nest the loops we could easily parallelize bematch_coordinates_skytween folders ;) \n",
    "# That can come later if necessary, it probably won't be. \n",
    "\n",
    "# Also parallel https://stackoverflow.com/questions/20548628/how-to-do-parallel-programming-in-python\n",
    "        \n",
    "#print(all_trans_matches)\n",
    "#cur.close()\n",
    "#tiles_path=\"/global/project/projectdirs/desi/spectro/redux/daily/tiles\"\n",
    "#run_path=\"/global/u2/p/palmese/desi/timedomain/cronjobs/\"\n",
    "#td_path=\"/global/cfs/cdirs/desi/science/td/daily-search/\"\n",
    "#mapfile -t -d $'\\n' obsdates_tileids < <( sqlite3 ${td_path}transients_search.db \"$query\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closer_check(matches_dict = {}, ledger_df = None, ledger_type = '', exclusion_list = []):\n",
    "    all_exp_matches = {}\n",
    "    #already_checked = []\n",
    "    \n",
    "    # easier way may be to query the sql table again and pop in the necessary information down the line\n",
    "    # Just a thought for cleanup time ;) \n",
    "    \n",
    "    if not matches_dict:\n",
    "        print(\"No far matches fed in for nearby matching. Returning none.\")\n",
    "        return {}\n",
    "    \n",
    "    if ledger_type.upper() == 'DECAM_TAMU':\n",
    "        if ledger_df.empty:\n",
    "            ledger_df = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "    \n",
    "    for date, row in matches_dict.items(): \n",
    "        print(\"\\n\", date)\n",
    "        if date in exclusion_list:\n",
    "            continue\n",
    "    #date = 20210404\n",
    "    #for row in [alert_matches_dict[date]]:\n",
    "\n",
    "        all_exp_matches[date] = []\n",
    "        alert_exp_matches = []\n",
    "        file_indices = {}\n",
    "\n",
    "        all_targ_ras = np.array([])\n",
    "        all_targ_decs = np.array([])\n",
    "\n",
    "        for i in row:\n",
    "            exp_paths = '/'.join((exposure_path, \"daily/exposures\", str(i['obsdate']), \"000\"+str(i['expid'])))\n",
    "            #print(exp_paths)\n",
    "            #all_exp_fits[date].extend()\n",
    "            for path in glob_frames(exp_paths):\n",
    "                #print(path)\n",
    "                targ_ras, targ_decs, _ = read_fits_ra_dec(path, False)\n",
    "\n",
    "                all_len = len(all_targ_ras)\n",
    "                new_len = len(targ_ras)\n",
    "                if all_len:\n",
    "                    all_len -= 1\n",
    "                    file_indices[path] = (all_len, all_len + new_len) # The start and end index, modulo number\n",
    "                else:\n",
    "                    file_indices[path] = (0, new_len) # The start and end index, modulo number\n",
    "\n",
    "                if len(targ_ras) != len(targ_decs):\n",
    "                    print(\"Length of all ras vs. all decs do not match.\")\n",
    "                    print(\"Something went wrong!\")\n",
    "                    print(\"Continuing but not adding those to match...\")\n",
    "                    continue\n",
    "\n",
    "                all_targ_ras = np.append(all_targ_ras, targ_ras)\n",
    "                all_targ_decs = np.append(all_targ_decs, targ_decs)\n",
    "\n",
    "        date_mjd = str(date)[:4]+\"-\"+str(date)[4:6] + \"-\" + str(date)[6:] # Adding dashes for Time\n",
    "        date_mjd = Time(date_mjd).mjd\n",
    "        \n",
    "        if ledger_type.upper() == 'ALERCE':\n",
    "            if ledger_df.empty:\n",
    "                ledger_df = access_alerts(lastmjd=obs_mjd-28) # Modified Julian Day #.mjd\n",
    "        \n",
    "        alert_exp_matches, alerts_matches = decam_matching(all_targ_ras, all_targ_decs, date_mjd, '', max_sep = 5, sep_units = 'arcsec', ledger_df_in = ledger_df, ledger_type_in = ledger_type)\n",
    "\n",
    "        #print(alert_exp_matches)\n",
    "        for match_idx in range(len(alert_exp_matches)):\n",
    "            match_ra = alert_exp_matches[match_idx].ra.deg\n",
    "            match_dec = alert_exp_matches[match_idx].dec.deg\n",
    "\n",
    "            alert_ra = alerts_matches[match_idx].ra.deg\n",
    "            alert_dec = alerts_matches[match_idx].dec.deg\n",
    "\n",
    "            loc = np.where(match_ra == all_targ_ras)[0][0] # Very very unlikely that we have exactly the same ra for two observations\n",
    "            #print(loc)\n",
    "            for k, v in file_indices.items():\n",
    "                if loc in range(v[0], v[1]):\n",
    "                    # filepath, (ra,dec) for match, loc + 1 because fits indexing starts at 1, (RA, DEC) for ledger table\n",
    "                    match_info = (k.split(\"/\")[-1], (match_ra, match_dec), (loc + 1) % (v[1] - v[0]), (alert_ra, alert_dec)) \n",
    "                    # Since it's a pain to retrieve the index from the table, it'll be easier to match it after the fact with np.where\n",
    "                    if match_info not in all_exp_matches[date]:\n",
    "                        all_exp_matches[date].append(match_info)\n",
    "            \n",
    "    return all_exp_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing\n",
    "for k, v in file_indices.items():\n",
    "    if loc in range(v[0], v[1]):\n",
    "        print()\n",
    "        # filepath, (ra,dec) for match, loc + 1 because fits indexing starts at 1, (RA, DEC) for alert table\n",
    "        match_info = (k.split(\"/\")[-1], (match_ra, match_dec), (loc + 1) % (v[1] - v[0]), (alert_ra, alert_dec)) \n",
    "        # Since it's a pain to retrieve the index from the table, it'll be easier to match it after the fact with np.where\n",
    "        #all_exp_matches[date].append(match_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_mjd = Time(\"2021-04-04\").mjd\n",
    "alerts = access_alerts(lastmjd=obs_mjd-28)\n",
    "#print(alerts)\n",
    "#alerts_ra = alerts['meanra'].to_numpy()\n",
    "#alerts_dec = alerts['meandec'].to_numpy()\n",
    "\n",
    "# Worry about accessing the data frame proper later, this is fine for now.\n",
    "#print(np.where(alerts_ra == match_info[-1][-2]))\n",
    "#print(np.where(alerts_dec == match_info[-1][-1]))\n",
    "#print(alerts_ra[298])\n",
    "#print(alerts_dec[298])\n",
    "#print(alerts_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.80676576666667\n",
      "51.1619776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "oid                  ZTF18aablddj\n",
       "ndethist                       21\n",
       "ncovhist                     1021\n",
       "mjdstarthist              58131.4\n",
       "mjdendhist                59310.3\n",
       "corrected                    True\n",
       "stellar                     False\n",
       "ndet                            3\n",
       "g_r_max                       NaN\n",
       "g_r_max_corr                  NaN\n",
       "g_r_mean                      NaN\n",
       "g_r_mean_corr                 NaN\n",
       "firstmjd                  58429.5\n",
       "lastmjd                   59310.3\n",
       "deltajd                   880.816\n",
       "meanra                    173.807\n",
       "meandec                    51.162\n",
       "sigmara               7.02346e-05\n",
       "sigmadec              0.000142533\n",
       "class                          SN\n",
       "classifier       stamp_classifier\n",
       "probability              0.466365\n",
       "step_id_corr     correction_0.0.1\n",
       "Name: 725, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(alert_ra)\n",
    "print(alert_dec)\n",
    "alerts.columns.values\n",
    "#list(alerts.index.values)\n",
    "alerts.iloc[np.where(pd.Index(alerts[\"meanra\"]) == alert_ra)[0][0]] # grab row from alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_matches_to_file(query_date_start, query_date_end, all_exp_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matches_file(filename: str) -> dict:\n",
    "    info_dict = {}\n",
    "    try:\n",
    "        with open(filename) as f:\n",
    "            all_lines = f.readlines()[2:] # Don't need header keywords - they're just there for humans (darned humans)\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not open or read:\", filename)\n",
    "#        print(\"Trying the next file...\")\n",
    "        return info_dict\n",
    "        \n",
    "    for line_idx in range(len(all_lines)):\n",
    "        #try:\n",
    "            #_ = int(all_lines[line_idx][:-1]) # to exclude \":\"\n",
    "        if \":\" in all_lines[line_idx]: # If even and 0\n",
    "            date = all_lines[line_idx].split(':')[0] # Gets rid of ':' and newline character\n",
    "            info_dict[date] = []\n",
    "        else:\n",
    "            data = all_lines[line_idx].lstrip('\\t').rstrip('\\n').split(';')\n",
    "            info_dict[date].extend(data)\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alert_matches_dict_2 = {i['obsdate']: [] for i in matches_list}\n",
    "\n",
    "for tup in all_trans_matches[0]:\n",
    "    ra = tup.ra.deg\n",
    "    dec = tup.dec.deg\n",
    "    print((ra,dec))\n",
    "    for date, row in date_dict.items():   \n",
    "        match_rows = [i for i in row if (i['tilera'], i['tiledec']) == (ra,dec)]\n",
    "        alert_matches_dict_2[date].extend(match_rows)\n",
    "print(alert_matches_dict_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#index = {k['obsdate'] : list(filter(lambda x:x in k, k)) for k in matches_list}\n",
    "index = {k['obsdate'] : list(filter(lambda x:x['obsdate'] == k['obsdate'], matches_list)) for k in matches_list}\n",
    "#print(len(matches_list))\n",
    "#print(list(index.keys()))\n",
    "tot = 0\n",
    "for obj,val in index.items():\n",
    "    for i in val:\n",
    "        print(i[:], end = \", \")\n",
    "    tot += len(index[obj])\n",
    "    print()\n",
    "    \n",
    "print(tot)\n",
    "#print(index)\n",
    "\n",
    "# https://stackoverflow.com/questions/11276473/append-to-a-dict-of-lists-with-a-dict-comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Match pointing (center of DESI circle, 1.8 deg radius)\n",
    "RA, DEC of pointing in cframe files~~\n",
    "\n",
    "~~Once match, run through CNN\n",
    "obsdate, tilenumber then runs on all fibers\n",
    "Force a result for those fiber(s) where match\n",
    "Grab coadd file on a match and then feed that to CNN (will ask for help)~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Notes for running the classifier \n",
    "Looks for broad spectrum as opposed to narrow emission line since we're looking at \\*novae - blackbody\n",
    "zbest has fibermap???\n",
    "Can remove (From py or ipynb)                 # Apply standard event selection.\n",
    "                isTGT = fibermap['OBJTYPE'] == 'TGT'\n",
    "                isGAL = zbest['SPECTYPE'] == 'GALAXY'\n",
    "Just look using fibernumber/fiberid (select = one fiber)\n",
    "idx = selects max across all categories for whole sample so just change that \n",
    "For applygradcam to one spectrum - rsflux[specific_index, :]\n",
    "https://github.com/desihub/timedomain/blob/master/desitrip/docs/nb/cnn_classify_data_gradCAM.ipynb~~\n",
    "\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECAM transients - https://github.com/desihub/timedomain/blob/master/too_ledgers/decam_ledgermaker.ipynb\n",
    "Transient name server reports only most interesting transients (brokers have more info)\n",
    "need RA-OBJECT and DEC-OBJECT and Discovery-Time (changes to be pushed by Antonella), it's a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decam_matching(target_ras_in = np.array([]), target_decs_in = np.array([]), obs_mjd_in = '', path_in = '', max_sep = 5, sep_units = 'arcsec', ledger_df_in = None, ledger_type_in = ''): # to be combined with the other matching thing in due time\n",
    "    \n",
    "    if sep_units == 'arcsec':\n",
    "        max_sep *= u.arcsec\n",
    "    elif sep_units == 'arcmin':\n",
    "        max_sep *= u.arcmin\n",
    "    elif sep_units == 'deg':\n",
    "        max_sep *= u.deg\n",
    "    else:\n",
    "        print(\"Separation unit specified is invalid for matching. Defaulting to arcsecond.\")\n",
    "        max_sep *= u.arcsec\n",
    "        \n",
    "    if not np.array(target_ras_in).size:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    nan_ra = np.isnan(target_ras_in)\n",
    "    nan_dec = np.isnan(target_decs_in)\n",
    "    \n",
    "    if np.any(nan_ra) or np.any(nan_dec):\n",
    "        print(\"NaNs found, removing them from array (not FITS) before match.\")\n",
    "        #print(\"Original length (ra, dec): \", len(target_ras), len(target_decs))\n",
    "        nans = np.logical_not(np.logical_and(nan_ra, nan_dec))\n",
    "        target_ras_in = target_ras_in[nans] # Logic masking, probably more efficient\n",
    "        target_decs_in = target_decs_in[nans]\n",
    "        #print(\"Reduced length (ra, dec):\", len(target_ras), len(target_decs))\n",
    "        #print(np.where(np.isnan(target_ras) == True))\n",
    "        #print(target_ras[:100])\n",
    "        #print(np.where(np.isnan(target_decs) == True))\n",
    "        #target_ras = np.delete(target_ras, nans)\n",
    "        #target_decs = np.delete(target_decs, nans)\n",
    "    \n",
    "    #alerts = access_alerts(lastmjd=obs_mjd-28) # Modified Julian Day #.mjd\n",
    "    # Write function to decide if alerce or DECAM\n",
    "    tree_name = \"_\".join((\"kdtree\", ledger_type_in, str(obs_mjd_in - 28)))\n",
    "    \n",
    "    if ledger_type_in.upper() == 'DECAM_TAMU':\n",
    "        ra_head = 'RA-OBJECT'\n",
    "        dec_head = 'DEC-OBJECT'\n",
    "    \n",
    "    elif ledger_type_in.upper() == 'ALERCE':\n",
    "        ra_head = 'meanra'\n",
    "        dec_head = 'meandec'\n",
    "        \n",
    "    else:\n",
    "        print(\"No ledger type specified. Will try to figure it out assuming it's a pandas dataframe.\")\n",
    "        print(\"Returning empty-handed for now until that is complete - Matthew\")\n",
    "        return np.array([]), np.array([])\n",
    "        #try: -- Do this later if necessary, try to find the right column name by searching for RA/DEC and then plug that in.\n",
    "        # To account for columns with RA and DEC in there that aren't what we need, compare the RA and DEC strings and look for the shortest\n",
    "        # In hopes that longer ones are for calibration and whatnot\n",
    "        #    if 'ra' or 'RA' in ledger_df.columns... something along these lines\n",
    "    \n",
    "    alerts_ra = ledger_df_in[ra_head].to_numpy()\n",
    "    alerts_dec = ledger_df_in[dec_head].to_numpy()\n",
    "\n",
    "    coo_trans_search = SkyCoord(target_ras_in*u.deg, target_decs_in*u.deg)\n",
    "    coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "\n",
    "    idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts, storekdtree = tree_name) # store tree to speed up subsequent results\n",
    "\n",
    "    sep_constraint = d2d_trans < max_sep\n",
    "    trans_matches = coo_trans_search[sep_constraint]\n",
    "    alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "    \n",
    "    if trans_matches.size:\n",
    "        sort_dist = np.sort(d2d_trans)\n",
    "        print(\"Minimum distance found: \", sort_dist[0])\n",
    "        #print(\"5 closest (in case there's more than one): \", sort_dist[:5])\n",
    "        #print()\n",
    "\n",
    "    #if trans_matches.size:\n",
    "        #all_trans_matches.append(trans_matches)\n",
    "        #all_alerts_matches.append(alerts_matches)\n",
    "\n",
    "    return trans_matches, alerts_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decam_transients = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ObjectID</th>\n",
       "      <th>RA-OBJECT</th>\n",
       "      <th>DEC-OBJECT</th>\n",
       "      <th>NumberAlerts</th>\n",
       "      <th>MaxSCORE</th>\n",
       "      <th>RA-PSEUDO-HOST</th>\n",
       "      <th>DEC-PSEUDO-HOST</th>\n",
       "      <th>SEP-PSEUDO-HOST</th>\n",
       "      <th>RA-NEIGHBOR-STAR</th>\n",
       "      <th>DEC-NEIGHBOR-STAR</th>\n",
       "      <th>...</th>\n",
       "      <th>Discovery-Round</th>\n",
       "      <th>Discovery-Time</th>\n",
       "      <th>Discovery-Filter</th>\n",
       "      <th>Discovery-Magnitude</th>\n",
       "      <th>Discovery-SNR</th>\n",
       "      <th>Latest-Round</th>\n",
       "      <th>Latest-Time</th>\n",
       "      <th>Latest-Filter</th>\n",
       "      <th>Latest-Magnitude</th>\n",
       "      <th>Latest-SNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221407558m001825</td>\n",
       "      <td>211.982786</td>\n",
       "      <td>-0.306951</td>\n",
       "      <td>12</td>\n",
       "      <td>0.972</td>\n",
       "      <td>211.982614</td>\n",
       "      <td>-0.306946</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>211.983372</td>\n",
       "      <td>-0.306315</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T06:40:19.074</td>\n",
       "      <td>N</td>\n",
       "      <td>22.13</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9</td>\n",
       "      <td>2021-04-18T05:37:55.763</td>\n",
       "      <td>N</td>\n",
       "      <td>22.86</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221408139m033502</td>\n",
       "      <td>212.057952</td>\n",
       "      <td>-3.583947</td>\n",
       "      <td>24</td>\n",
       "      <td>0.953</td>\n",
       "      <td>212.057864</td>\n",
       "      <td>-3.583960</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>212.058798</td>\n",
       "      <td>-3.586276</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:20:58.209</td>\n",
       "      <td>N</td>\n",
       "      <td>21.78</td>\n",
       "      <td>25.7</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T05:05:47.308</td>\n",
       "      <td>N</td>\n",
       "      <td>22.30</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221408412p002445</td>\n",
       "      <td>212.171737</td>\n",
       "      <td>0.412527</td>\n",
       "      <td>36</td>\n",
       "      <td>0.998</td>\n",
       "      <td>212.171673</td>\n",
       "      <td>0.412394</td>\n",
       "      <td>0.5317</td>\n",
       "      <td>212.174697</td>\n",
       "      <td>0.411566</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T06:36:50.928</td>\n",
       "      <td>S</td>\n",
       "      <td>20.61</td>\n",
       "      <td>36.2</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T04:56:22.444</td>\n",
       "      <td>S</td>\n",
       "      <td>21.34</td>\n",
       "      <td>24.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221408578m005300</td>\n",
       "      <td>212.241200</td>\n",
       "      <td>-0.883300</td>\n",
       "      <td>2</td>\n",
       "      <td>0.855</td>\n",
       "      <td>212.241200</td>\n",
       "      <td>-0.883400</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>212.239800</td>\n",
       "      <td>-0.884900</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:17:30.880</td>\n",
       "      <td>S</td>\n",
       "      <td>22.55</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-24T07:10:51.368</td>\n",
       "      <td>S</td>\n",
       "      <td>22.33</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221409059m023156</td>\n",
       "      <td>212.274757</td>\n",
       "      <td>-2.532478</td>\n",
       "      <td>20</td>\n",
       "      <td>0.969</td>\n",
       "      <td>212.274533</td>\n",
       "      <td>-2.532531</td>\n",
       "      <td>0.8290</td>\n",
       "      <td>212.275356</td>\n",
       "      <td>-2.535003</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:14:02.747</td>\n",
       "      <td>N</td>\n",
       "      <td>22.27</td>\n",
       "      <td>10.6</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T05:08:44.659</td>\n",
       "      <td>S</td>\n",
       "      <td>21.87</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202105031415178p020156</td>\n",
       "      <td>213.824507</td>\n",
       "      <td>2.032258</td>\n",
       "      <td>2</td>\n",
       "      <td>0.827</td>\n",
       "      <td>213.823644</td>\n",
       "      <td>2.032113</td>\n",
       "      <td>3.1500</td>\n",
       "      <td>213.822730</td>\n",
       "      <td>2.034362</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T03:32:55.455</td>\n",
       "      <td>S</td>\n",
       "      <td>21.40</td>\n",
       "      <td>25.8</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T03:34:14.026</td>\n",
       "      <td>S</td>\n",
       "      <td>21.47</td>\n",
       "      <td>24.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202105031422035p013900</td>\n",
       "      <td>215.514867</td>\n",
       "      <td>1.650119</td>\n",
       "      <td>2</td>\n",
       "      <td>0.808</td>\n",
       "      <td>215.515314</td>\n",
       "      <td>1.650044</td>\n",
       "      <td>1.6302</td>\n",
       "      <td>215.512864</td>\n",
       "      <td>1.651186</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T03:38:16.342</td>\n",
       "      <td>S</td>\n",
       "      <td>22.11</td>\n",
       "      <td>15.1</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T03:39:34.760</td>\n",
       "      <td>S</td>\n",
       "      <td>22.10</td>\n",
       "      <td>14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202105031435063p004233</td>\n",
       "      <td>218.776650</td>\n",
       "      <td>0.709182</td>\n",
       "      <td>2</td>\n",
       "      <td>0.720</td>\n",
       "      <td>218.776316</td>\n",
       "      <td>0.709057</td>\n",
       "      <td>1.2821</td>\n",
       "      <td>218.775068</td>\n",
       "      <td>0.709650</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T04:18:41.070</td>\n",
       "      <td>N</td>\n",
       "      <td>22.35</td>\n",
       "      <td>10.4</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T04:40:07.656</td>\n",
       "      <td>S</td>\n",
       "      <td>22.10</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202105031441256p023213</td>\n",
       "      <td>220.356979</td>\n",
       "      <td>2.537110</td>\n",
       "      <td>2</td>\n",
       "      <td>0.933</td>\n",
       "      <td>220.356686</td>\n",
       "      <td>2.537058</td>\n",
       "      <td>1.0696</td>\n",
       "      <td>220.358840</td>\n",
       "      <td>2.538181</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T04:24:01.515</td>\n",
       "      <td>S</td>\n",
       "      <td>22.54</td>\n",
       "      <td>8.7</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T04:25:39.552</td>\n",
       "      <td>S</td>\n",
       "      <td>21.37</td>\n",
       "      <td>11.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T202105031451333m023945</td>\n",
       "      <td>222.888870</td>\n",
       "      <td>-2.662617</td>\n",
       "      <td>2</td>\n",
       "      <td>0.728</td>\n",
       "      <td>222.889215</td>\n",
       "      <td>-2.663304</td>\n",
       "      <td>2.7662</td>\n",
       "      <td>222.890861</td>\n",
       "      <td>-2.663219</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T06:44:28.745</td>\n",
       "      <td>S</td>\n",
       "      <td>21.10</td>\n",
       "      <td>14.2</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-05-03T06:45:52.410</td>\n",
       "      <td>S</td>\n",
       "      <td>21.20</td>\n",
       "      <td>18.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ObjectID   RA-OBJECT  DEC-OBJECT  NumberAlerts  MaxSCORE  \\\n",
       "0   A202103221407558m001825  211.982786   -0.306951            12     0.972   \n",
       "0   A202103221408139m033502  212.057952   -3.583947            24     0.953   \n",
       "0   A202103221408412p002445  212.171737    0.412527            36     0.998   \n",
       "0   A202103221408578m005300  212.241200   -0.883300             2     0.855   \n",
       "0   A202103221409059m023156  212.274757   -2.532478            20     0.969   \n",
       "..                      ...         ...         ...           ...       ...   \n",
       "0   T202105031415178p020156  213.824507    2.032258             2     0.827   \n",
       "0   T202105031422035p013900  215.514867    1.650119             2     0.808   \n",
       "0   T202105031435063p004233  218.776650    0.709182             2     0.720   \n",
       "0   T202105031441256p023213  220.356979    2.537110             2     0.933   \n",
       "0   T202105031451333m023945  222.888870   -2.662617             2     0.728   \n",
       "\n",
       "    RA-PSEUDO-HOST  DEC-PSEUDO-HOST  SEP-PSEUDO-HOST  RA-NEIGHBOR-STAR  \\\n",
       "0       211.982614        -0.306946           0.6199        211.983372   \n",
       "0       212.057864        -3.583960           0.3199        212.058798   \n",
       "0       212.171673         0.412394           0.5317        212.174697   \n",
       "0       212.241200        -0.883400           0.3000        212.239800   \n",
       "0       212.274533        -2.532531           0.8290        212.275356   \n",
       "..             ...              ...              ...               ...   \n",
       "0       213.823644         2.032113           3.1500        213.822730   \n",
       "0       215.515314         1.650044           1.6302        215.512864   \n",
       "0       218.776316         0.709057           1.2821        218.775068   \n",
       "0       220.356686         2.537058           1.0696        220.358840   \n",
       "0       222.889215        -2.663304           2.7662        222.890861   \n",
       "\n",
       "    DEC-NEIGHBOR-STAR  ...  Discovery-Round           Discovery-Time  \\\n",
       "0           -0.306315  ...                0  2021-03-22T06:40:19.074   \n",
       "0           -3.586276  ...                0  2021-03-22T08:20:58.209   \n",
       "0            0.411566  ...                0  2021-03-22T06:36:50.928   \n",
       "0           -0.884900  ...                0  2021-03-22T08:17:30.880   \n",
       "0           -2.535003  ...                0  2021-03-22T08:14:02.747   \n",
       "..                ...  ...              ...                      ...   \n",
       "0            2.034362  ...               13  2021-05-03T03:32:55.455   \n",
       "0            1.651186  ...               13  2021-05-03T03:38:16.342   \n",
       "0            0.709650  ...               13  2021-05-03T04:18:41.070   \n",
       "0            2.538181  ...               13  2021-05-03T04:24:01.515   \n",
       "0           -2.663219  ...               13  2021-05-03T06:44:28.745   \n",
       "\n",
       "   Discovery-Filter Discovery-Magnitude  Discovery-SNR  Latest-Round  \\\n",
       "0                 N               22.13           19.2             9   \n",
       "0                 N               21.78           25.7            13   \n",
       "0                 S               20.61           36.2            13   \n",
       "0                 S               22.55           15.5             1   \n",
       "0                 N               22.27           10.6            13   \n",
       "..              ...                 ...            ...           ...   \n",
       "0                 S               21.40           25.8            13   \n",
       "0                 S               22.11           15.1            13   \n",
       "0                 N               22.35           10.4            13   \n",
       "0                 S               22.54            8.7            13   \n",
       "0                 S               21.10           14.2            13   \n",
       "\n",
       "                Latest-Time Latest-Filter Latest-Magnitude  Latest-SNR  \n",
       "0   2021-04-18T05:37:55.763             N            22.86        10.2  \n",
       "0   2021-05-03T05:05:47.308             N            22.30        12.0  \n",
       "0   2021-05-03T04:56:22.444             S            21.34        24.3  \n",
       "0   2021-03-24T07:10:51.368             S            22.33        25.6  \n",
       "0   2021-05-03T05:08:44.659             S            21.87         8.5  \n",
       "..                      ...           ...              ...         ...  \n",
       "0   2021-05-03T03:34:14.026             S            21.47        24.9  \n",
       "0   2021-05-03T03:39:34.760             S            22.10        14.4  \n",
       "0   2021-05-03T04:40:07.656             S            22.10        11.4  \n",
       "0   2021-05-03T04:25:39.552             S            21.37        11.7  \n",
       "0   2021-05-03T06:45:52.410             S            21.20        18.1  \n",
       "\n",
       "[345 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decam_transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_matches_by_date = initial_check(ledger_df = decam_transients, ledger_type = 'DECAM_TAMU')\n",
    "#matches, alert_matches = decam_matching(target_ras = [], target_decs = [], obs_mjd = '', path_in = '', max_sep = 5, sep_units = 'arcsec', ledger_df = [], ledger_type = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 20210101\n",
      "\n",
      " 20210115\n",
      "\n",
      " 20210205\n",
      "\n",
      " 20210208\n",
      "\n",
      " 20210217\n",
      "\n",
      " 20210218\n",
      "\n",
      " 20210221\n",
      "Minimum distance found:  0d00m03.784s\n",
      "\n",
      " 20210322\n",
      "Minimum distance found:  0d00m03.784s\n",
      "\n",
      " 20210402\n",
      "\n",
      " 20210405\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.2082s\n",
      "\n",
      " 20210406\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3198s\n",
      "\n",
      " 20210407\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3034s\n",
      "\n",
      " 20210408\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.5481s\n",
      "\n",
      " 20210409\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3075s\n",
      "\n",
      " 20210410\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.2082s\n",
      "\n",
      " 20210411\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "\n",
      " 20210412\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.6055s\n",
      "\n",
      " 20210413\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.4077s\n",
      "\n",
      " 20210414\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3358s\n",
      "\n",
      " 20210415\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.4221s\n",
      "\n",
      " 20210416\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.2082s\n",
      "\n",
      " 20210417\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.4701s\n",
      "\n",
      " 20210418\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.6694s\n",
      "\n",
      " 20210419\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.0938s\n",
      "\n",
      " 20210420\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3637s\n",
      "\n",
      " 20210428\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "\n",
      " 20210429\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.6694s\n",
      "\n",
      " 20210430\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210501\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3872s\n",
      "\n",
      " 20210502\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210503\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210504\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210505\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00s\n"
     ]
    }
   ],
   "source": [
    "exclusion_list = [20210101, 20210115, 20210205, 20210208, 20210217, 20210218, 20210402,20210411, 20210428]\n",
    "close_matches = closer_check(init_matches_by_date, ledger_df = decam_transients, ledger_type = 'DECAM_TAMU', exclusion_list = exclusion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "#print({k:v for k, v in close_matches.items() if v})\n",
    "tot = 0\n",
    "for date, val in close_matches.items():\n",
    "    #close_matches[date] = list(set(val))\n",
    "    tot += len(val)\n",
    "print(tot)\n",
    "#print(len(close_matches.values()))\n",
    "#write_matches_to_file(\"20201130\", \"20210505\", close_matches, \"DECAM_TAMU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80662 20210221 3  348\n",
      "80662 20210322 3  348\n",
      "228 20210405 5  112\n",
      "217 20210406 3  309\n",
      "28 20210407 6  101\n",
      "246 20210408 8  76\n",
      "219 20210409 3  81\n",
      "228 20210410 2  450\n",
      "221 20210412 4  151\n",
      "283 20210413 9  257\n",
      "284 20210414 9  372\n",
      "43 20210415 1  205\n",
      "285 20210416 5  218\n",
      "229 20210417 5  333\n",
      "260 20210418 5  406\n",
      "252 20210419 8  394\n",
      "230 20210420 0  396\n",
      "260 20210429 5  406\n",
      "287 20210430 8  284\n",
      "47 20210501 6  321\n",
      "262 20210502 8  341\n",
      "262 20210503 6  216\n",
      "513 20210504 8  251\n",
      "289 20210505 8  219\n"
     ]
    }
   ],
   "source": [
    "file_dict = read_matches_file(glob.glob(\"./matches/*.txt\")[1])\n",
    "#print(file_dict)\n",
    "\n",
    "query_template = \"SELECT distinct obsdate, tileid from exposures where expid == \" #obsdate>20210228 \n",
    "\n",
    "#query2 = \"PRAGMA table_info(exposures)\"\n",
    "\n",
    "#cur.execute(query2)\n",
    "#row2 = cur.fetchall()\n",
    "#for i in row2:\n",
    "#    print(i[:])\n",
    "\n",
    "conn = sqlite3.connect(db_filename)\n",
    "conn.row_factory = sqlite3.Row # https://docs.python.org/3/library/sqlite3.html#sqlite3.Row\n",
    "cur = conn.cursor()\n",
    "\n",
    "for date, v in file_dict.items():\n",
    "    #print(v)\n",
    "    exp_id = v[0].strip(\"\\t\").split('-')[-1][3:-5] # [:-5] to avoid retained '.fits' at end\n",
    "    row = v[2]\n",
    "    #print(v)\n",
    "    #print(exp_id)\n",
    "    petal_num = v[0].split(\"-\")[1][1] # First split \"cframe, [color_band][petal_num], [exp_id].fits\"\n",
    "    query = query_template + exp_id + \";\"\n",
    "    \n",
    "    cur.execute(query)\n",
    "    tile_id = cur.fetchone()['tileid']\n",
    "    #target_id = cur.fetchone()['targetid'] # Grab targetid from cframe file row specified in file\n",
    "    \n",
    "    #print(date, tile_id, row)\n",
    "    # See if you can't grab the targetid in modified_cnn_classify while opening zbest using info\n",
    "    # from matches_decam file since we're already accessing zbest\n",
    "    # Check header of zbest files per cframe name/expid/tileid for targetid\n",
    "    # save yourself the trouble ;)\n",
    "    \n",
    "    #print('python3 cnn_classify_data.py -d {} -t {} -g'.format(date, tile_id))\n",
    "    \n",
    "    #coadd_filename = \"-\".join((\"coadd\", petal_num, str(row_data['tileid']), date)) + \".fits\"\n",
    "    zbest_filename = \"-\".join((\"zbest\", petal_num, str(tile_id), date)) + \".fits\"\n",
    "    print(tile_id, date, petal_num, row)\n",
    "    \n",
    "    #coadd_filepath = '/'.join((exposure_path, \"daily/tiles\", str(row_data['tileid']), date, coadd_filename)) #coadd-7-81088-20210404.fits\n",
    "    #zbest_filepath = '/'.join((exposure_path, \"daily/tiles\", str(row_data['tileid']), date, zbest_filename)) #zbest-7-81088-20210404.fits\n",
    "    \n",
    "    # Next up - figure out how to feed this to CNN!\n",
    "    #print(coadd_filepath)\n",
    "\n",
    "    #with fits.open(coadd_filepath) as hdu1:\n",
    "        #data_table = Table(hdu1[hdu_num].data) #columns\n",
    "\n",
    "    #targ_id = data_table['TARGETID']\n",
    "    #targ_ra = data_table['TARGET_RA'].data # Now it's a numpy array\n",
    "    #targ_dec = data_table['TARGET_DEC'].data\n",
    "    #targ_mjd = data_table['MJD'][0] some have different versions of this so this is a *bad* idea... at least now I know the try except works!\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes to self - double matches are to be expected, could be worthwhile to compare the spectrum to both\n",
    "# next time setup pipeline to find these individual spectra or at least lists or something to export to other ipynb to then run as a loop\n",
    "# may be tough because... you know... ipynb... we'll see!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESI master",
   "language": "python",
   "name": "desi-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
