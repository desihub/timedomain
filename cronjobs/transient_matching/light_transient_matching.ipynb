{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install --user alerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import astropy.config\n",
    "astropy.config.get_cache_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy import units as u\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord, match_coordinates_sky, Angle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "global db_filename\n",
    "db_filename = '/global/cfs/cdirs/desi/science/td/daily-search/transients_search.db'\n",
    "global exposure_path\n",
    "exposure_path = os.environ[\"DESI_SPECTRO_REDUX\"]\n",
    "global color_band\n",
    "color_band = \"r\"\n",
    "\n",
    "global today\n",
    "today = Time.now()\n",
    "#import warnings\n",
    "#from ALeRCE_ledgermaker import access_alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the file names\n",
    "def all_candidate_filenames(transient_dir: str):\n",
    "    \n",
    "    # This function grabs the names of all input files in the transient directory and does some python string manipulation\n",
    "    # to grab the names of the input files with full path and the filenames themselves.\n",
    "\n",
    "    #transient_dir = \n",
    "    try:\n",
    "        filenames_read = glob.glob(transient_dir + \"/*.fits\") # Hardcoding is hopefully a temporary measure.\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not grab/find any fits in the transient spectra directory:\")\n",
    "        print(transient_dir)\n",
    "        filenames_read = [] # Just in case\n",
    "        #filenames_out = [] # Just in case\n",
    "        raise SystemExit(\"Exitting.\")\n",
    "        \n",
    "    #else:\n",
    "        #filenames_out = [s.split(\".\")[0] for s in filenames_read]\n",
    "        #filenames_out = [s.split(\"/\")[-1] for s in filenames_read]\n",
    "        #filenames_out = [s.replace(\"in\", \"out\") for s in filenames_out]\n",
    "        \n",
    "    return filenames_read #, filenames_out\n",
    "\n",
    "#path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "#print(all_candidate_filenames(path_to_transient)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From ALeRCE_ledgermaker https://github.com/alercebroker/alerce_client\n",
    "# I have had trouble importing this before so here it goes\n",
    "# Now modified ***\n",
    "import requests\n",
    "#import matplotlib as mpl\n",
    "#import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "from alerce.core import Alerce\n",
    "from alerce.exceptions import APIError\n",
    "\n",
    "alerce_client = Alerce()\n",
    "\n",
    "def access_alerts(lastmjd_in=None, classifier='stamp_classifier', class_names=['SN', 'AGN']):\n",
    "    if type(class_names) is not list:\n",
    "        raise TypeError('Argument `class_names` must be a list.')\n",
    "        \n",
    "    dataframes = []\n",
    "    if not lastmjd_in:\n",
    "        date_range = 60\n",
    "        lastmjd_in = Time.now().mjd - 60\n",
    "        print('Defaulting to a lastmjd range of', str(date_range), 'days before today.')\n",
    "        \n",
    "    for class_name in class_names:\n",
    "        data = alerce_client.query_objects(classifier=classifier,\n",
    "                                           class_name=class_name, \n",
    "                                           order_by='oid',\n",
    "                                           order_mode='DESC',\n",
    "                                           page_size=5000,\n",
    "                                           lastmjd=lastmjd_in,\n",
    "                                           format='pandas')\n",
    "        \n",
    "        #if lastmjd is not None:\n",
    "        #    select = data['lastmjd'] >= lastmjd\n",
    "        #    data = data[select]\n",
    "            \n",
    "        dataframes.append(data)\n",
    "    \n",
    "    return pd.concat(dataframes).sort_values(by='lastmjd')#(by='lastmjd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/desihub/timedomain/blob/master/too_ledgers/decam_TAMU_ledgermaker.ipynb\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "def access_decam_data(url, overwrite=False):\n",
    "    \"\"\"Download reduced DECam transient data from Texas A&M.\n",
    "    Cache the data to avoid lengthy and expensive downloads.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL for accessing the data.\n",
    "    overwrite : bool\n",
    "        Download new data and overwrite the cached data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    decam_transients : pandas.DataFrame\n",
    "        Table of transient data.\n",
    "    \"\"\"\n",
    "    folders = url.split('/')\n",
    "    thedate = folders[-1] if len(folders[-1]) > 0 else folders[-2]\n",
    "    outfile = '{}.csv'.format(thedate)\n",
    "    \n",
    "    if os.path.exists(outfile) and not overwrite:\n",
    "        # Access cached data.\n",
    "        decam_transients = pd.read_csv(outfile)\n",
    "    else:\n",
    "        # Download the DECam data index.\n",
    "        # A try/except is needed because the datahub SSL certificate isn't playing well with URL requests.\n",
    "        try:\n",
    "            decam_dets = requests.get(url, auth=('decam','tamudecam')).text\n",
    "        except:\n",
    "            requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)\n",
    "            decam_dets = requests.get(url, verify=False, auth=('decam','tamudecam')).text\n",
    "            \n",
    "        # Convert transient index page into scrapable data using BeautifulSoup.\n",
    "        soup = BeautifulSoup(decam_dets)\n",
    "        \n",
    "        # Loop through transient object summary JSON files indexed in the main transient page.\n",
    "        # Download the JSONs and dump the info into a Pandas table.\n",
    "        decam_transients = None\n",
    "        j = 0\n",
    "\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            if 'object-summary.json' in a:\n",
    "                link = a['href'].replace('./', '')\n",
    "                summary_url  = url + link        \n",
    "                summary_text = requests.get(summary_url, verify=False, auth=('decam','tamudecam')).text\n",
    "                summary_data = json.loads(summary_text)\n",
    "\n",
    "                j += 1\n",
    "                #print('Accessing {:3d}  {}'.format(j, summary_url)) # Modified by Matt\n",
    "\n",
    "                if decam_transients is None:\n",
    "                    decam_transients = pd.DataFrame(summary_data, index=[0])\n",
    "                else:\n",
    "                    decam_transients = pd.concat([decam_transients, pd.DataFrame(summary_data, index=[0])])\n",
    "                    \n",
    "        # Cache the data for future access.\n",
    "        print('Saving output to {}'.format(outfile))\n",
    "        decam_transients.to_csv(outfile, index=False)\n",
    "        \n",
    "    return decam_transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fits_ra_dec(filepath: str, transient_candidate = True):\n",
    "    \n",
    "    if transient_candidate:\n",
    "        hdu_num = 1\n",
    "    else:\n",
    "        hdu_num = 5\n",
    "    \n",
    "    try:\n",
    "        with fits.open(filepath) as hdu1:\n",
    "    \n",
    "            data_table = Table(hdu1[hdu_num].data) #columns\n",
    "        \n",
    "            #targ_id = data_table['TARGETID']\n",
    "            targ_ra = data_table['TARGET_RA'].data # Now it's a numpy array\n",
    "            targ_dec = data_table['TARGET_DEC'].data\n",
    "            #targ_mjd = data_table['MJD'][0] some have different versions of this so this is a *bad* idea... at least now I know the try except works!\n",
    "            \n",
    "            if not transient_candidate:\n",
    "                targ_mjd = hdu1[hdu_num].header['MJD-OBS']\n",
    "            \n",
    "    except:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        print(\"Could not open or use:\", filename)\n",
    "        print(\"In path:\", filepath)\n",
    "        print(\"Trying the next file...\")\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    if transient_candidate:\n",
    "        targ_mjd = filepath.split(\"/\")[-1].split(\"_\")[-2] #to grab the date\n",
    "        targ_mjd = targ_mjd[:4]+\"-\"+targ_mjd[4:6]+\"-\"+targ_mjd[6:] # Adding dashes for Time\n",
    "        targ_mjd = Time(targ_mjd).mjd\n",
    "    \n",
    "    return targ_ra, targ_dec, targ_mjd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(path_in: str, max_sep: float, transient_cand = True, target_ra_dec_date = ()): #, #first_run: bool = True): # To be cleaned up...\n",
    "    \n",
    "    max_sep *= u.arcsec\n",
    "    \n",
    "    if not target_ra_dec_date:\n",
    "        target_ras, target_decs, obs_mjd = read_fits_ra_dec(path_in, transient_cand)\n",
    "    else:\n",
    "        target_ras, target_decs, obs_mjd = target_ra_dec_date\n",
    "        \n",
    "    if not target_ras.size:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    nan_ra = np.isnan(target_ras)\n",
    "    nan_dec = np.isnan(target_decs)\n",
    "    \n",
    "    if np.any(nan_ra) or np.any(nan_dec):\n",
    "        print(\"NaNs found, removing them from array (not FITS) before match.\")\n",
    "        #print(\"Original length (ra, dec): \", len(target_ras), len(target_decs))\n",
    "        nans = np.logical_not(np.logical_and(nan_ra, nan_dec))\n",
    "        target_ras = target_ras[nans] # Logic masking, probably more efficient\n",
    "        target_decs = target_decs[nans]\n",
    "        #print(\"Reduced length (ra, dec):\", len(target_ras), len(target_decs))\n",
    "        #print(np.where(np.isnan(target_ras) == True))\n",
    "        #print(target_ras[:100])\n",
    "        #print(np.where(np.isnan(target_decs) == True))\n",
    "        #target_ras = np.delete(target_ras, nans)\n",
    "        #target_decs = np.delete(target_decs, nans)\n",
    "    \n",
    "    alerts = access_alerts(lastmjd_in=obs_mjd - 28) # Modified Julian Day #.mjd\n",
    "    # Write function to decide if alerce or DECAM\n",
    "    tree_name = \"kdtree_\" + str(obs_mjd - 28)\n",
    "    \n",
    "    # For each fits file, look at one month before the observation from Alerce\n",
    "    alerts_ra = alerts['meanra'].to_numpy()\n",
    "    alerts_dec = alerts['meandec'].to_numpy()\n",
    "\n",
    "    coo_trans_search = SkyCoord(target_ras*u.deg, target_decs*u.deg)\n",
    "    #print(coo_trans_search)\n",
    "    coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "\n",
    "    idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts, storekdtree = tree_name) # store tree to speed up subsequent results\n",
    "\n",
    "    sep_constraint = d2d_trans < max_sep\n",
    "    trans_matches = coo_trans_search[sep_constraint]\n",
    "    alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "    \n",
    "    if trans_matches.size:\n",
    "        sort_dist = np.sort(d2d_trans)\n",
    "        print(\"Minimum distance found: \", sort_dist[0])\n",
    "        print(\"5 closest (in case there's more than one): \", sort_dist[:5])\n",
    "        print()\n",
    "\n",
    "    #if trans_matches.size:\n",
    "        #all_trans_matches.append(trans_matches)\n",
    "        #all_alerts_matches.append(alerts_matches)\n",
    "\n",
    "    return trans_matches, alerts_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def matching(path_in: str, max_sep: float):\n",
    "    \n",
    "    max_sep *= u.arcsec\n",
    "    \n",
    "    target_ras, target_decs, obs_date = read_fits_ra_dec(path_in)\n",
    "    \n",
    "    filename = path_in.split(\"/\")[-1]\n",
    "    tree_name = \"kdtree_\" + filename[:-5] # Gets rid of the .fits at the end\n",
    "        \n",
    "    if not target_ras.size:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    alerts = access_alerts(lastmjd=obs_date-28) # Modified Julian Day #.mjd\n",
    "    # For each fits file, look at one month before the observation from Alerce\n",
    "    alerts_ra = alerts['meanra'].to_numpy()\n",
    "    alerts_dec = alerts['meandec'].to_numpy()\n",
    "\n",
    "    coo_trans_search = SkyCoord(target_ras*u.deg, target_decs*u.deg)\n",
    "    #print(coo_trans_search)\n",
    "    coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "    \n",
    "    print(tree_name)\n",
    "    idx_trans, d2d_alerts, d3d_alerts = match_coordinates_sky(coo_alerts, coo_trans_search, storekdtree = tree_name) # Store tree to speed up subsequent results\n",
    "\n",
    "    sep_constraint = d2d_alerts < max_sep\n",
    "    alerts_matches = coo_alerts[sep_constraint]\n",
    "    trans_matches = coo_trans_search[idx_trans[sep_constraint]]\n",
    "\n",
    "    #if trans_matches.size:\n",
    "        #all_trans_matches.append(trans_matches)\n",
    "        #all_alerts_matches.append(alerts_matches)\n",
    "\n",
    "    return trans_matches, alerts_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def check_corners(fits_path, ):\n",
    "\n",
    "    paths_to_fits = all_candidate_filenames(fits_path)\n",
    "    target_ra, target_dec, obs_date = read_fits_ra_dec(paths_to_fits)\n",
    "\n",
    "    min_ra_idx = np.argmin(target_ra) # If we use data_table['TARGET_RA'].data it'll convert the data into a numpy array\n",
    "    max_ra_idx = np.argmax(target_ra)\n",
    "    min_dec_idx = np.argmin(target_dec)\n",
    "    max_dec_idx = np.argmax(target_dec) # What if it's inside the area and the area is larger than 5\"? If/then statement!\n",
    "\n",
    "    min_max_ra = [target_ra[min_ra_idx], target_ra[max_ra_idx], target_ra[min_dec_idx], target_ra[max_dec_idx]]\n",
    "    min_max_dec = [target_dec[min_ra_idx], target_dec[max_ra_idx], target_dec[min_dec_idx], target_dec[max_dec_idx]]\n",
    "\n",
    "    min_max = SkyCoord(min_max_ra*u.deg, min_max_dec*u.deg)\n",
    "\n",
    "    idx, sep2d, d3d = match_coordinates_sky(min_max, min_max, 2)\n",
    "\n",
    "    max_dist = max(sep2d)\n",
    "    # Optimal - combine all frames into one giant numpy array and check the four corners of those - then do this on the alerts, if you find nothing, good, move on\n",
    "    # If you do find something, figure out which one its closest to by the index and do that\n",
    "    #print(d3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "{'Error code': 500, 'Message': 'Internal Server Error', 'Data': {}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9d908e2ad2dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths_to_fits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtrans_matches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malerts_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#         target_ras, target_decs, target_ids = read_fits_ra_dec(path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-08e52e95eb71>\u001b[0m in \u001b[0;36mmatching\u001b[0;34m(path_in, max_sep, transient_cand, target_ra_dec_date)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#target_decs = np.delete(target_decs, nans)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0malerts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccess_alerts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlastmjd_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs_mjd\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Modified Julian Day #.mjd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Write function to decide if alerce or DECAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtree_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"kdtree_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_mjd\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4d86b96af013>\u001b[0m in \u001b[0;36maccess_alerts\u001b[0;34m(lastmjd_in, classifier, class_names)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         data = alerce_client.query_objects(classifier=classifier,\n\u001b[0m\u001b[1;32m     25\u001b[0m                                            \u001b[0mclass_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                            \u001b[0morder_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'oid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/alerce/search.py\u001b[0m in \u001b[0;36mquery_objects\u001b[0;34m(self, format, index, sort, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"class_name\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"class_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         q = self._request(\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"objects\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/alerce/utils.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, params, response_field, result_format)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mhandle_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse_field\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult_format\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse_field\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/alerce/exceptions.py\u001b[0m in \u001b[0;36mhandle_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     raise codes.get(code, APIError)(\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;31mAPIError\u001b[0m: {'Error code': 500, 'Message': 'Internal Server Error', 'Data': {}}"
     ]
    }
   ],
   "source": [
    "# Reading FITS and grabbing RA and DEC\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #alerts_coords = np.array(zip(alerts['meanra'].to_numpy(), alerts['meandec'].to_numpy()))\n",
    "    \n",
    "    path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "    paths_to_fits = all_candidate_filenames(path_to_transient)\n",
    "    #fits_image_filename = fits.util.get_testdata_filepath(fits_name[0])\n",
    "    \n",
    "    # If done in serial, would it be faster to chunk all of the ra's and dec's together before comparing? Actually maybe not because of kdtree.\n",
    "    # If done in parallel though, I doubt it.\n",
    "    \n",
    "#     max_sep = 5.0 * u.arcsec\n",
    "    \n",
    "    all_trans_matches = []\n",
    "    all_alerts_matches = []\n",
    "    \n",
    "    #trans_match, alerts_match = matching(paths_to_fits[0], 5.0) # To get the kdtree started *shrugs*\n",
    "    #if trans_match.size:\n",
    "    #    all_trans_matches.append(trans_match)\n",
    "    #    all_alerts_matches.append(alerts_match)\n",
    "        \n",
    "    for path in paths_to_fits[:2]:\n",
    "        trans_matches, alerts_matches = matching(path, 5.0)\n",
    "#         target_ras, target_decs, target_ids = read_fits_ra_dec(path)\n",
    "        \n",
    "#         if not target_ras.size:\n",
    "#             continue\n",
    "\n",
    "#         coo_trans_search = SkyCoord(targ_ra*u.deg, targ_dec*u.deg)\n",
    "#         coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "\n",
    "#         idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts, storekdtree= True) #'kdtree_alerts') # store tree to speed up subsequent results\n",
    "\n",
    "#         sep_constraint = d2d_trans < max_sep\n",
    "#         trans_matches = coo_trans_search[sep_constraint]\n",
    "#         alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "        #print(trans_matches)\n",
    "        if trans_matches.size:\n",
    "            all_trans_matches.append(trans_matches)\n",
    "            all_alerts_matches.append(alerts_matches)\n",
    "        \n",
    "    print(all_trans_matches)\n",
    "\n",
    "    #print(sorted(set(idx_trans)))\n",
    "    #print(d2d_trans.arcsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_frames(exp_d: str):\n",
    "    # Grabbing the frame fits files\n",
    "    \n",
    "    # This function grabs the names of all input files in the transient directory and does some python string manipulation\n",
    "    # to grab the names of the input files with full path and the filenames themselves.\n",
    "\n",
    "    try:\n",
    "        filenames_read = glob.glob(exp_d + \"/cframe-\" + color_band + \"*.fits\") # Only need one of b, r, z\n",
    "        # sframes not flux calibrated\n",
    "        # May want to use tiles... coadd (will need later, but not now)\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not grab/find any fits in the exposure directory:\")\n",
    "        print(exp_d)\n",
    "        filenames_read = [] # Just in case\n",
    "        #filenames_out = [] # Just in case\n",
    "        raise SystemExit(\"Exitting.\")\n",
    "        \n",
    "    #else:\n",
    "        #filenames_out = [s.split(\".\")[0] for s in filenames_read]\n",
    "        #filenames_out = [s.split(\"/\")[-1] for s in filenames_read]\n",
    "        #filenames_out = [s.replace(\"in\", \"out\") for s in filenames_out]\n",
    "        \n",
    "    return filenames_read #, filenames_out\n",
    "\n",
    "#path_to_transient = \"/global/cfs/cdirs/desi/science/td/daily-search/desitrip/out\"\n",
    "#print(all_candidate_filenames(path_to_transient)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_matches_to_file(start_date, end_date, all_matches_dict, ledger_type):\n",
    "    filename = \"./matches_\" + ledger_type + \"_\" + start_date + \"-\" + end_date + \".txt\"\n",
    "    with open(filename, 'w') as mfile:\n",
    "        mfile.write(\"date:\\n\")\n",
    "        mfile.write(\"\\tframename; (RA, DEC); Table Index in FIBERMAP; Ledger ID; Alert (RA, DEC) - matched to 2\\\"\\n\") # Add fiber id, distance between TAMU and fiber obj\n",
    "        for key, val in all_matches_dict.items():\n",
    "            if val:\n",
    "                mfile.write(str(key) + \": \\n\")\n",
    "                for vals in val:\n",
    "                    mfile.write(\"\\t\")\n",
    "                    #print(\", \".join(vals))\n",
    "                    mfile.write(\"; \".join(str(x) for x in vals))\n",
    "                    mfile.write(\"\\n\")\n",
    "        #for match in exp_matches:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_check(ledger_df = None, ledger_type = ''):\n",
    "\n",
    "    #for obsdate,tile_number in obsdates_tilenumbers:\n",
    "        #redux = '/'.join([os.environ['DESI_SPECTRO_REDUX'], args.redux, 'tiles'])\n",
    "        #prefix_in = '/'.join([redux, tile_number, obsdate])\n",
    "\n",
    "    # previous_date_ranges: \"20201130\" - \"20210302\"\n",
    "    query_date_start = \"20201130\" #\"20210228\"\n",
    "    \n",
    "    today = Time.now()\n",
    "    smushed_YMD = today.iso.split(\" \")[0].replace(\"-\",\"\")\n",
    "    \n",
    "    query_date_end = smushed_YMD #\"20210505\" #\"20210501\"\n",
    "\n",
    "    query2 = \"PRAGMA table_info(exposures)\"\n",
    "    query3 = \"PRAGMA table_info(tiles)\"\n",
    "    # Crossmatch across tiles and exposures to grab obsdate via tileid\n",
    "    query_match = \"SELECT distinct tilera, tiledec, obsdate, obsmjd, expid, exposures.tileid from exposures INNER JOIN tiles ON exposures.tileid = tiles.tileid where obsdate BETWEEN \" + \\\n",
    "        query_date_start + \" AND \" + query_date_end + \";\" #obsdate>20210228 \n",
    "    #query_dates = \"SELECT obsdate from exposures INNER JOIN tiles ON exposures.tileid = tiles.tileid where obsdate BETWEEN \" + \\\n",
    "        #query_date_start + \" AND \" + query_date_end + \";\"\n",
    "\n",
    "    #cur.execute(query2)\n",
    "    #row2 = cur.fetchall()\n",
    "    #for i in row2:\n",
    "    #    print(i[:])\n",
    "\n",
    "    conn = sqlite3.connect(db_filename)\n",
    "\n",
    "    conn.row_factory = sqlite3.Row # https://docs.python.org/3/library/sqlite3.html#sqlite3.Row\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    #cur.execute(query)\n",
    "    #rows = cur.fetchall()\n",
    "\n",
    "    cur.execute(query_match)\n",
    "    matches_list = cur.fetchall()\n",
    "    cur.close()\n",
    "\n",
    "    # I knew there was a way! THANK YOU!\n",
    "    # https://stackoverflow.com/questions/11276473/append-to-a-dict-of-lists-with-a-dict-comprehension\n",
    "    date_dict = {k['obsdate'] : list(filter(lambda x:x['obsdate'] == k['obsdate'], matches_list)) for k in matches_list}\n",
    "\n",
    "    #cur.execute(query_dates)\n",
    "    #dates = set(cur.fetchall())\n",
    "\n",
    "    alert_matches_dict = {} #{i['obsdate']: [] for i in matches_list}\n",
    "\n",
    "    all_trans_matches = []\n",
    "    all_alerts_matches = []\n",
    "    \n",
    "    #decam_transients = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "    if ledger_type.upper() == 'DECAM_TAMU':\n",
    "        if ledger_df.empty:\n",
    "            ledger_df = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "\n",
    "    for date, row in date_dict.items():\n",
    "        \n",
    "        date_str = str(date)\n",
    "        date_str = date_str[:4]+\"-\"+date_str[4:6]+\"-\"+date_str[6:] # Adding dashes for Time\n",
    "        obs_mjd = Time(date_str).mjd\n",
    "\n",
    "        # This method is *technically* safer than doing a double list comprehension with set albeit slower\n",
    "        # The lists are small enough that speed shouldn't matter here\n",
    "        unique_tileid = {i[-1]:(i[0], i[1]) for i in row}\n",
    "        #set([(i[0], i[1]) for i in row]) # there's probably a way to do this in SQL... oh well\n",
    "        #print(unique_ra_dec)\n",
    "        exposure_ras, exposure_decs = zip(*unique_tileid.values())\n",
    "        \n",
    "        if ledger_type.upper() == 'ALERCE':\n",
    "            if ledger_df.empty:\n",
    "                ledger_df = access_alerts(lastmjd=obs_mjd-28) # Modified Julian Day #.mjd\n",
    "        elif ledger_type.upper() == 'DECAM_TAMU':\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Cannot use alerts broker/ledger provided. Stopping before match.\")\n",
    "            return {}\n",
    "        \n",
    "        #print(target_ras)\n",
    "        trans_matches, alert_matches = decam_matching(target_ras, target_decs, obs_mjd, '', max_sep = 1.8, sep_units = 'deg', ledger_df_in = ledger_df, ledger_type_in = ledger_type)\n",
    "        \n",
    "        if trans_matches.size:\n",
    "            #print(len(row))\n",
    "            all_trans_matches.append(trans_matches)\n",
    "            # To retrieve from SkyCoord into numpy float, x.ra.deg, x.dec.deg\n",
    "            all_alerts_matches.append(alert_matches)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        alert_matches_dict[date] = []\n",
    "\n",
    "        for tup in trans_matches:\n",
    "            ra = tup.ra.deg\n",
    "            dec = tup.dec.deg\n",
    "            match_rows = [i for i in row if (i['tilera'], i['tiledec']) == (ra, dec)]\n",
    "            alert_matches_dict[date].extend(match_rows)\n",
    "            \n",
    "    return alert_matches_dict\n",
    "            \n",
    "            \n",
    "#print(alert_matches_dict)\n",
    "        \n",
    "# I bet if we nest the loops we could easily parallelize bematch_coordinates_skytween folders ;) \n",
    "# That can come later if necessary, it probably won't be. \n",
    "\n",
    "# Also parallel https://stackoverflow.com/questions/20548628/how-to-do-parallel-programming-in-python\n",
    "        \n",
    "#print(all_trans_matches)\n",
    "#cur.close()\n",
    "#tiles_path=\"/global/project/projectdirs/desi/spectro/redux/daily/tiles\"\n",
    "#run_path=\"/global/u2/p/palmese/desi/timedomain/cronjobs/\"\n",
    "#td_path=\"/global/cfs/cdirs/desi/science/td/daily-search/\"\n",
    "#mapfile -t -d $'\\n' obsdates_tileids < <( sqlite3 ${td_path}transients_search.db \"$query\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closer_check(matches_dict = {}, ledger_df = None, ledger_type = '', exclusion_list = []):\n",
    "    all_exp_matches = {}\n",
    "    #already_checked = []\n",
    "    \n",
    "    # easier way may be to query the sql table again and pop in the necessary information down the line\n",
    "    # Just a thought for cleanup time ;) \n",
    "    \n",
    "    if not matches_dict:\n",
    "        print(\"No far matches fed in for nearby matching. Returning none.\")\n",
    "        return {}\n",
    "    \n",
    "    if ledger_type.upper() == 'DECAM_TAMU':\n",
    "        \n",
    "        id_head = 'ObjectID'\n",
    "        ra_head = 'RA-OBJECT'\n",
    "        dec_head = 'DEC-OBJECT'\n",
    "        \n",
    "        if ledger_df.empty:\n",
    "            ledger_df = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/')\n",
    "    \n",
    "    for date, row in matches_dict.items(): \n",
    "        print(\"\\n\", date)\n",
    "        if date in exclusion_list:\n",
    "            continue\n",
    "    #date = 20210404\n",
    "    #for row in [alert_matches_dict[date]]:\n",
    "\n",
    "        all_exp_matches[date] = []\n",
    "        alert_exp_matches = []\n",
    "        file_indices = {}\n",
    "\n",
    "        all_targ_ras = np.array([])\n",
    "        all_targ_decs = np.array([])\n",
    "\n",
    "        for i in row:\n",
    "            exp_paths = '/'.join((exposure_path, \"daily/exposures\", str(i['obsdate']), \"000\"+str(i['expid'])))\n",
    "            #print(exp_paths)\n",
    "            #all_exp_fits[date].extend()\n",
    "            for path in glob_frames(exp_paths):\n",
    "                #print(path)\n",
    "                targ_ras, targ_decs, _ = read_fits_ra_dec(path, False)\n",
    "\n",
    "                all_len = len(all_targ_ras)\n",
    "                new_len = len(targ_ras)\n",
    "                if all_len:\n",
    "                    all_len -= 1\n",
    "                    file_indices[path] = (all_len, all_len + new_len) # The start and end index, modulo number\n",
    "                else:\n",
    "                    file_indices[path] = (0, new_len) # The start and end index, modulo number\n",
    "\n",
    "                if len(targ_ras) != len(targ_decs):\n",
    "                    print(\"Length of all ras vs. all decs do not match.\")\n",
    "                    print(\"Something went wrong!\")\n",
    "                    print(\"Continuing but not adding those to match...\")\n",
    "                    continue\n",
    "\n",
    "                all_targ_ras = np.append(all_targ_ras, targ_ras)\n",
    "                all_targ_decs = np.append(all_targ_decs, targ_decs)\n",
    "\n",
    "        date_mjd = str(date)[:4]+\"-\"+str(date)[4:6] + \"-\" + str(date)[6:] # Adding dashes for Time\n",
    "        date_mjd = Time(date_mjd).mjd\n",
    "        \n",
    "        if ledger_type.upper() == 'ALERCE':\n",
    "            \n",
    "            id_head = 'oid'\n",
    "            ra_head = 'meanra'\n",
    "            dec_head = 'meandec'\n",
    "            \n",
    "            if ledger_df.empty:\n",
    "                ledger_df = access_alerts(lastmjd_in=obs_mjd-28) # Modified Julian Day #.mjd\n",
    "        \n",
    "        alert_exp_matches, alerts_matches = decam_matching(all_targ_ras, all_targ_decs, date_mjd, '', max_sep = 2, sep_units = 'arcsec', ledger_df_in = ledger_df, ledger_type_in = ledger_type)\n",
    "\n",
    "        #print(alert_exp_matches)\n",
    "        for match_idx in range(len(alert_exp_matches)):\n",
    "            match_ra = alert_exp_matches[match_idx].ra.deg\n",
    "            match_dec = alert_exp_matches[match_idx].dec.deg\n",
    "            \n",
    "            location = np.where(match_ra == all_targ_ras)[0][0] # VERY unlikely to have a duplicate in the RA, I think this is safe\n",
    "            \n",
    "            alert_ra = alerts_matches[match_idx].ra.deg\n",
    "            alert_dec = alerts_matches[match_idx].dec.deg\n",
    "            \n",
    "            # From meanra column, match ra, then grab the location, specify the key 'oid', grab the values from that 'series'\n",
    "            # should only have one match so we can grab the first\n",
    "            ledger_ID = ledger_df.loc[ledger_df[ra_head] == alert_ra][id_head].values[0] # I think this too is safe           \n",
    "            \n",
    "            #print(loc)\n",
    "            for k, v in file_indices.items():\n",
    "                if location in range(v[0], v[1]):\n",
    "                    # filepath, (ra,dec) for match, loc + 1 because fits indexing starts at 1, Ledger ID, (RA, DEC) for ledger table\n",
    "                    match_info = (k.split(\"/\")[-1], (match_ra, match_dec), (loc + 1) % (v[1] - v[0]), ledger_ID, (alert_ra, alert_dec)) \n",
    "                    # Since it's a pain to retrieve the index from the table, it'll be easier to match it after the fact with np.where\n",
    "                    if match_info not in all_exp_matches[date]:\n",
    "                        all_exp_matches[date].append(match_info)\n",
    "            \n",
    "    return all_exp_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing\n",
    "for k, v in file_indices.items():\n",
    "    if loc in range(v[0], v[1]):\n",
    "        print()\n",
    "        # filepath, (ra,dec) for match, loc + 1 because fits indexing starts at 1, (RA, DEC) for alert table\n",
    "        match_info = (k.split(\"/\")[-1], (match_ra, match_dec), (loc + 1) % (v[1] - v[0]), (alert_ra, alert_dec)) \n",
    "        # Since it's a pain to retrieve the index from the table, it'll be easier to match it after the fact with np.where\n",
    "        #all_exp_matches[date].append(match_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "access_alerts() got an unexpected keyword argument 'lastmjd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-61fb208f7563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobs_mjd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2021-04-04\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmjd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0malerts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccess_alerts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlastmjd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs_mjd\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(alerts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#alerts_ra = alerts['meanra'].to_numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#alerts_dec = alerts['meandec'].to_numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: access_alerts() got an unexpected keyword argument 'lastmjd'"
     ]
    }
   ],
   "source": [
    "obs_mjd = Time(\"2021-04-04\").mjd\n",
    "alerts = access_alerts(lastmjd_in=obs_mjd-28)\n",
    "#print(alerts)\n",
    "#alerts_ra = alerts['meanra'].to_numpy()\n",
    "#alerts_dec = alerts['meandec'].to_numpy()\n",
    "\n",
    "# Worry about accessing the data frame proper later, this is fine for now.\n",
    "#print(np.where(alerts_ra == match_info[-1][-2]))\n",
    "#print(np.where(alerts_dec == match_info[-1][-1]))\n",
    "#print(alerts_ra[298])\n",
    "#print(alerts_dec[298])\n",
    "#print(alerts_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alert_ra)\n",
    "print(alert_dec)\n",
    "alerts.columns.values\n",
    "#list(alerts.index.values)\n",
    "alerts.iloc[np.where(pd.Index(alerts[\"meanra\"]) == alert_ra)[0][0]] # grab row from alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_matches_to_file(query_date_start, query_date_end, all_exp_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matches_file(filename: str) -> dict:\n",
    "    info_dict = {}\n",
    "    try:\n",
    "        with open(filename) as f:\n",
    "            all_lines = f.readlines()[2:] # Don't need header keywords - they're just there for humans (darned humans)\n",
    "    \n",
    "    except:\n",
    "        print(\"Could not open or read:\", filename)\n",
    "#        print(\"Trying the next file...\")\n",
    "        return info_dict\n",
    "        \n",
    "    for line_idx in range(len(all_lines)):\n",
    "        #try:\n",
    "            #_ = int(all_lines[line_idx][:-1]) # to exclude \":\"\n",
    "        if \":\" in all_lines[line_idx]: # If even and 0\n",
    "            date = all_lines[line_idx].split(':')[0] # Gets rid of ':' and newline character\n",
    "            info_dict[date] = {}\n",
    "        else:\n",
    "            data = all_lines[line_idx].lstrip('\\t').rstrip('\\n').replace(\" \", \"\").split(';')\n",
    "            info_dict[date][data[0]] = data[1:] #.extend(data)\n",
    "    \n",
    "    return info_dict # now a dict of dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alert_matches_dict_2 = {i['obsdate']: [] for i in matches_list}\n",
    "\n",
    "for tup in all_trans_matches[0]:\n",
    "    ra = tup.ra.deg\n",
    "    dec = tup.dec.deg\n",
    "    print((ra,dec))\n",
    "    for date, row in date_dict.items():   \n",
    "        match_rows = [i for i in row if (i['tilera'], i['tiledec']) == (ra,dec)]\n",
    "        alert_matches_dict_2[date].extend(match_rows)\n",
    "print(alert_matches_dict_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#index = {k['obsdate'] : list(filter(lambda x:x in k, k)) for k in matches_list}\n",
    "index = {k['obsdate'] : list(filter(lambda x:x['obsdate'] == k['obsdate'], matches_list)) for k in matches_list}\n",
    "#print(len(matches_list))\n",
    "#print(list(index.keys()))\n",
    "tot = 0\n",
    "for obj,val in index.items():\n",
    "    for i in val:\n",
    "        print(i[:], end = \", \")\n",
    "    tot += len(index[obj])\n",
    "    print()\n",
    "    \n",
    "print(tot)\n",
    "#print(index)\n",
    "\n",
    "# https://stackoverflow.com/questions/11276473/append-to-a-dict-of-lists-with-a-dict-comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Match pointing (center of DESI circle, 1.8 deg radius)\n",
    "RA, DEC of pointing in cframe files~~\n",
    "\n",
    "~~Once match, run through CNN\n",
    "obsdate, tilenumber then runs on all fibers\n",
    "Force a result for those fiber(s) where match\n",
    "Grab coadd file on a match and then feed that to CNN (will ask for help)~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Notes for running the classifier \n",
    "Looks for broad spectrum as opposed to narrow emission line since we're looking at \\*novae - blackbody\n",
    "zbest has fibermap???\n",
    "Can remove (From py or ipynb)                 # Apply standard event selection.\n",
    "                isTGT = fibermap['OBJTYPE'] == 'TGT'\n",
    "                isGAL = zbest['SPECTYPE'] == 'GALAXY'\n",
    "Just look using fibernumber/fiberid (select = one fiber)\n",
    "idx = selects max across all categories for whole sample so just change that \n",
    "For applygradcam to one spectrum - rsflux[specific_index, :]\n",
    "https://github.com/desihub/timedomain/blob/master/desitrip/docs/nb/cnn_classify_data_gradCAM.ipynb~~\n",
    "\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECAM transients - https://github.com/desihub/timedomain/blob/master/too_ledgers/decam_ledgermaker.ipynb\n",
    "Transient name server reports only most interesting transients (brokers have more info)\n",
    "need RA-OBJECT and DEC-OBJECT and Discovery-Time (changes to be pushed by Antonella), it's a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decam_matching(target_ras_in = np.array([]), target_decs_in = np.array([]), obs_mjd_in = '', path_in = '', max_sep = 2, sep_units = 'arcsec', ledger_df_in = None, ledger_type_in = ''): # to be combined with the other matching thing in due time\n",
    "    \n",
    "    if sep_units == 'arcsec':\n",
    "        max_sep *= u.arcsec\n",
    "    elif sep_units == 'arcmin':\n",
    "        max_sep *= u.arcmin\n",
    "    elif sep_units == 'deg':\n",
    "        max_sep *= u.deg\n",
    "    else:\n",
    "        print(\"Separation unit specified is invalid for matching. Defaulting to arcsecond.\")\n",
    "        max_sep *= u.arcsec\n",
    "        \n",
    "    if not np.array(target_ras_in).size:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    nan_ra = np.isnan(target_ras_in)\n",
    "    nan_dec = np.isnan(target_decs_in)\n",
    "    \n",
    "    if np.any(nan_ra) or np.any(nan_dec):\n",
    "        print(\"NaNs found, removing them from array (not FITS) before match.\")\n",
    "        #print(\"Original length (ra, dec): \", len(target_ras), len(target_decs))\n",
    "        nans = np.logical_not(np.logical_and(nan_ra, nan_dec))\n",
    "        target_ras_in = target_ras_in[nans] # Logic masking, probably more efficient\n",
    "        target_decs_in = target_decs_in[nans]\n",
    "        #print(\"Reduced length (ra, dec):\", len(target_ras), len(target_decs))\n",
    "        #print(np.where(np.isnan(target_ras) == True))\n",
    "        #print(target_ras[:100])\n",
    "        #print(np.where(np.isnan(target_decs) == True))\n",
    "        #target_ras = np.delete(target_ras, nans)\n",
    "        #target_decs = np.delete(target_decs, nans)\n",
    "    \n",
    "    #alerts = access_alerts(lastmjd=obs_mjd-28) # Modified Julian Day #.mjd\n",
    "    # Write function to decide if alerce or DECAM\n",
    "    tree_name = \"_\".join((\"kdtree\", ledger_type_in, str(obs_mjd_in - 28)))\n",
    "    \n",
    "    if ledger_type_in.upper() == 'DECAM_TAMU':\n",
    "        ra_head = 'RA-OBJECT'\n",
    "        dec_head = 'DEC-OBJECT'\n",
    "    \n",
    "    elif ledger_type_in.upper() == 'ALERCE':\n",
    "        ra_head = 'meanra'\n",
    "        dec_head = 'meandec'\n",
    "        \n",
    "    else:\n",
    "        print(\"No ledger type specified. Will try to figure it out assuming it's a pandas dataframe.\")\n",
    "        print(\"Returning empty-handed for now until that is complete - Matthew\")\n",
    "        return np.array([]), np.array([])\n",
    "        #try: -- Do this later if necessary, try to find the right column name by searching for RA/DEC and then plug that in.\n",
    "        # To account for columns with RA and DEC in there that aren't what we need, compare the RA and DEC strings and look for the shortest\n",
    "        # In hopes that longer ones are for calibration and whatnot\n",
    "        #    if 'ra' or 'RA' in ledger_df.columns... something along these lines\n",
    "    \n",
    "    alerts_ra = ledger_df_in[ra_head].to_numpy()\n",
    "    alerts_dec = ledger_df_in[dec_head].to_numpy()\n",
    "\n",
    "    coo_trans_search = SkyCoord(target_ras_in*u.deg, target_decs_in*u.deg)\n",
    "    coo_alerts = SkyCoord(alerts_ra*u.deg, alerts_dec*u.deg)\n",
    "\n",
    "    idx_alerts, d2d_trans, d3d_trans = match_coordinates_sky(coo_trans_search, coo_alerts, storekdtree = tree_name) # store tree to speed up subsequent results\n",
    "\n",
    "    sep_constraint = d2d_trans < max_sep\n",
    "    trans_matches = coo_trans_search[sep_constraint]\n",
    "    alerts_matches = coo_alerts[idx_alerts[sep_constraint]]\n",
    "    \n",
    "    if trans_matches.size:\n",
    "        sort_dist = np.sort(d2d_trans)\n",
    "        print(\"Minimum distance found: \", sort_dist[0])\n",
    "        #print(\"5 closest (in case there's more than one): \", sort_dist[:5])\n",
    "        #print()\n",
    "\n",
    "    #if trans_matches.size:\n",
    "        #all_trans_matches.append(trans_matches)\n",
    "        #all_alerts_matches.append(alerts_matches)\n",
    "\n",
    "    return trans_matches, alerts_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to figure out a way to put in username and password via Requests\n",
    "decam_transients = access_decam_data('https://datahub.geos.tamu.edu:8000/decam/LCData_Legacy/', overwrite = False) # If True, grabs a fresh batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ObjectID</th>\n",
       "      <th>RA-OBJECT</th>\n",
       "      <th>DEC-OBJECT</th>\n",
       "      <th>NumberAlerts</th>\n",
       "      <th>MaxSCORE</th>\n",
       "      <th>RA-PSEUDO-HOST</th>\n",
       "      <th>DEC-PSEUDO-HOST</th>\n",
       "      <th>SEP-PSEUDO-HOST</th>\n",
       "      <th>RA-NEIGHBOR-STAR</th>\n",
       "      <th>DEC-NEIGHBOR-STAR</th>\n",
       "      <th>...</th>\n",
       "      <th>Discovery-Round</th>\n",
       "      <th>Discovery-Time</th>\n",
       "      <th>Discovery-Filter</th>\n",
       "      <th>Discovery-Magnitude</th>\n",
       "      <th>Discovery-SNR</th>\n",
       "      <th>Latest-Round</th>\n",
       "      <th>Latest-Time</th>\n",
       "      <th>Latest-Filter</th>\n",
       "      <th>Latest-Magnitude</th>\n",
       "      <th>Latest-SNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A202103221407558m001825</td>\n",
       "      <td>211.982786</td>\n",
       "      <td>-0.306951</td>\n",
       "      <td>12</td>\n",
       "      <td>0.972</td>\n",
       "      <td>211.982614</td>\n",
       "      <td>-0.306946</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>211.983372</td>\n",
       "      <td>-0.306315</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T06:40:19.074</td>\n",
       "      <td>N</td>\n",
       "      <td>22.13</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9</td>\n",
       "      <td>2021-04-18T05:37:55.763</td>\n",
       "      <td>N</td>\n",
       "      <td>22.86</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A202103221408139m033502</td>\n",
       "      <td>212.057952</td>\n",
       "      <td>-3.583947</td>\n",
       "      <td>26</td>\n",
       "      <td>0.953</td>\n",
       "      <td>212.057864</td>\n",
       "      <td>-3.583960</td>\n",
       "      <td>0.3199</td>\n",
       "      <td>212.058798</td>\n",
       "      <td>-3.586276</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:20:58.209</td>\n",
       "      <td>N</td>\n",
       "      <td>21.78</td>\n",
       "      <td>25.7</td>\n",
       "      <td>18</td>\n",
       "      <td>2021-05-18T06:16:52.581</td>\n",
       "      <td>N</td>\n",
       "      <td>22.10</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A202103221408412p002445</td>\n",
       "      <td>212.171737</td>\n",
       "      <td>0.412527</td>\n",
       "      <td>47</td>\n",
       "      <td>0.998</td>\n",
       "      <td>212.171673</td>\n",
       "      <td>0.412394</td>\n",
       "      <td>0.5317</td>\n",
       "      <td>212.174697</td>\n",
       "      <td>0.411566</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T06:36:50.928</td>\n",
       "      <td>S</td>\n",
       "      <td>20.61</td>\n",
       "      <td>36.2</td>\n",
       "      <td>19</td>\n",
       "      <td>2021-05-22T03:10:29.715</td>\n",
       "      <td>S</td>\n",
       "      <td>21.53</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A202103221408578m005300</td>\n",
       "      <td>212.241200</td>\n",
       "      <td>-0.883300</td>\n",
       "      <td>2</td>\n",
       "      <td>0.855</td>\n",
       "      <td>212.241200</td>\n",
       "      <td>-0.883400</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>212.239800</td>\n",
       "      <td>-0.884900</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:17:30.880</td>\n",
       "      <td>S</td>\n",
       "      <td>22.55</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-24T07:10:51.368</td>\n",
       "      <td>S</td>\n",
       "      <td>22.33</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A202103221409059m023156</td>\n",
       "      <td>212.274757</td>\n",
       "      <td>-2.532478</td>\n",
       "      <td>21</td>\n",
       "      <td>0.969</td>\n",
       "      <td>212.274533</td>\n",
       "      <td>-2.532531</td>\n",
       "      <td>0.8290</td>\n",
       "      <td>212.275356</td>\n",
       "      <td>-2.535003</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-22T08:14:02.747</td>\n",
       "      <td>N</td>\n",
       "      <td>22.27</td>\n",
       "      <td>10.6</td>\n",
       "      <td>18</td>\n",
       "      <td>2021-05-18T03:17:17.544</td>\n",
       "      <td>N</td>\n",
       "      <td>22.99</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>T202105301405547p043407</td>\n",
       "      <td>211.477939</td>\n",
       "      <td>4.568626</td>\n",
       "      <td>2</td>\n",
       "      <td>0.881</td>\n",
       "      <td>211.473443</td>\n",
       "      <td>4.568954</td>\n",
       "      <td>16.1787</td>\n",
       "      <td>211.477656</td>\n",
       "      <td>4.568650</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T02:59:57.315</td>\n",
       "      <td>N</td>\n",
       "      <td>20.73</td>\n",
       "      <td>10.4</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T04:39:26.875</td>\n",
       "      <td>N</td>\n",
       "      <td>22.46</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>T202105301432117p040917</td>\n",
       "      <td>218.048953</td>\n",
       "      <td>4.154764</td>\n",
       "      <td>2</td>\n",
       "      <td>0.480</td>\n",
       "      <td>218.032649</td>\n",
       "      <td>4.182142</td>\n",
       "      <td>114.6331</td>\n",
       "      <td>218.047316</td>\n",
       "      <td>4.153485</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T05:38:41.744</td>\n",
       "      <td>S</td>\n",
       "      <td>21.74</td>\n",
       "      <td>10.8</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T06:16:17.835</td>\n",
       "      <td>S</td>\n",
       "      <td>21.69</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>T202105301434113p034128</td>\n",
       "      <td>218.547346</td>\n",
       "      <td>3.691341</td>\n",
       "      <td>2</td>\n",
       "      <td>0.702</td>\n",
       "      <td>218.549157</td>\n",
       "      <td>3.689940</td>\n",
       "      <td>8.2327</td>\n",
       "      <td>218.547538</td>\n",
       "      <td>3.691805</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T05:38:41.744</td>\n",
       "      <td>S</td>\n",
       "      <td>21.04</td>\n",
       "      <td>23.9</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T06:16:17.835</td>\n",
       "      <td>S</td>\n",
       "      <td>21.19</td>\n",
       "      <td>18.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>T202105301452527p011841</td>\n",
       "      <td>223.219769</td>\n",
       "      <td>1.311619</td>\n",
       "      <td>3</td>\n",
       "      <td>0.997</td>\n",
       "      <td>223.218727</td>\n",
       "      <td>1.310085</td>\n",
       "      <td>6.6763</td>\n",
       "      <td>223.219019</td>\n",
       "      <td>1.313305</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T05:17:31.450</td>\n",
       "      <td>N</td>\n",
       "      <td>20.18</td>\n",
       "      <td>38.6</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T06:32:46.888</td>\n",
       "      <td>N</td>\n",
       "      <td>20.25</td>\n",
       "      <td>32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>T202105301455348p012600</td>\n",
       "      <td>223.895405</td>\n",
       "      <td>1.433544</td>\n",
       "      <td>3</td>\n",
       "      <td>0.861</td>\n",
       "      <td>223.895024</td>\n",
       "      <td>1.433341</td>\n",
       "      <td>1.5555</td>\n",
       "      <td>223.892937</td>\n",
       "      <td>1.429588</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T05:17:31.450</td>\n",
       "      <td>N</td>\n",
       "      <td>21.89</td>\n",
       "      <td>8.7</td>\n",
       "      <td>22</td>\n",
       "      <td>2021-05-30T06:32:46.888</td>\n",
       "      <td>N</td>\n",
       "      <td>21.41</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ObjectID   RA-OBJECT  DEC-OBJECT  NumberAlerts  MaxSCORE  \\\n",
       "0    A202103221407558m001825  211.982786   -0.306951            12     0.972   \n",
       "1    A202103221408139m033502  212.057952   -3.583947            26     0.953   \n",
       "2    A202103221408412p002445  212.171737    0.412527            47     0.998   \n",
       "3    A202103221408578m005300  212.241200   -0.883300             2     0.855   \n",
       "4    A202103221409059m023156  212.274757   -2.532478            21     0.969   \n",
       "..                       ...         ...         ...           ...       ...   \n",
       "525  T202105301405547p043407  211.477939    4.568626             2     0.881   \n",
       "526  T202105301432117p040917  218.048953    4.154764             2     0.480   \n",
       "527  T202105301434113p034128  218.547346    3.691341             2     0.702   \n",
       "528  T202105301452527p011841  223.219769    1.311619             3     0.997   \n",
       "529  T202105301455348p012600  223.895405    1.433544             3     0.861   \n",
       "\n",
       "     RA-PSEUDO-HOST  DEC-PSEUDO-HOST  SEP-PSEUDO-HOST  RA-NEIGHBOR-STAR  \\\n",
       "0        211.982614        -0.306946           0.6199        211.983372   \n",
       "1        212.057864        -3.583960           0.3199        212.058798   \n",
       "2        212.171673         0.412394           0.5317        212.174697   \n",
       "3        212.241200        -0.883400           0.3000        212.239800   \n",
       "4        212.274533        -2.532531           0.8290        212.275356   \n",
       "..              ...              ...              ...               ...   \n",
       "525      211.473443         4.568954          16.1787        211.477656   \n",
       "526      218.032649         4.182142         114.6331        218.047316   \n",
       "527      218.549157         3.689940           8.2327        218.547538   \n",
       "528      223.218727         1.310085           6.6763        223.219019   \n",
       "529      223.895024         1.433341           1.5555        223.892937   \n",
       "\n",
       "     DEC-NEIGHBOR-STAR  ...  Discovery-Round           Discovery-Time  \\\n",
       "0            -0.306315  ...                0  2021-03-22T06:40:19.074   \n",
       "1            -3.586276  ...                0  2021-03-22T08:20:58.209   \n",
       "2             0.411566  ...                0  2021-03-22T06:36:50.928   \n",
       "3            -0.884900  ...                0  2021-03-22T08:17:30.880   \n",
       "4            -2.535003  ...                0  2021-03-22T08:14:02.747   \n",
       "..                 ...  ...              ...                      ...   \n",
       "525           4.568650  ...               22  2021-05-30T02:59:57.315   \n",
       "526           4.153485  ...               22  2021-05-30T05:38:41.744   \n",
       "527           3.691805  ...               22  2021-05-30T05:38:41.744   \n",
       "528           1.313305  ...               22  2021-05-30T05:17:31.450   \n",
       "529           1.429588  ...               22  2021-05-30T05:17:31.450   \n",
       "\n",
       "    Discovery-Filter Discovery-Magnitude  Discovery-SNR  Latest-Round  \\\n",
       "0                  N               22.13           19.2             9   \n",
       "1                  N               21.78           25.7            18   \n",
       "2                  S               20.61           36.2            19   \n",
       "3                  S               22.55           15.5             1   \n",
       "4                  N               22.27           10.6            18   \n",
       "..               ...                 ...            ...           ...   \n",
       "525                N               20.73           10.4            22   \n",
       "526                S               21.74           10.8            22   \n",
       "527                S               21.04           23.9            22   \n",
       "528                N               20.18           38.6            22   \n",
       "529                N               21.89            8.7            22   \n",
       "\n",
       "                 Latest-Time Latest-Filter Latest-Magnitude  Latest-SNR  \n",
       "0    2021-04-18T05:37:55.763             N            22.86        10.2  \n",
       "1    2021-05-18T06:16:52.581             N            22.10        13.9  \n",
       "2    2021-05-22T03:10:29.715             S            21.53        16.5  \n",
       "3    2021-03-24T07:10:51.368             S            22.33        25.6  \n",
       "4    2021-05-18T03:17:17.544             N            22.99         8.8  \n",
       "..                       ...           ...              ...         ...  \n",
       "525  2021-05-30T04:39:26.875             N            22.46        12.7  \n",
       "526  2021-05-30T06:16:17.835             S            21.69        10.3  \n",
       "527  2021-05-30T06:16:17.835             S            21.19        18.7  \n",
       "528  2021-05-30T06:32:46.888             N            20.25        32.6  \n",
       "529  2021-05-30T06:32:46.888             N            21.41        11.8  \n",
       "\n",
       "[530 rows x 21 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decam_transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T202104271211352p000120', 'T202104271213064p001246', 'T202104271217522m005428', 'T202104291421131p004942', 'T202105031359093p063735', 'T202105031415178p020156', 'T202105031422035p013900', 'T202105031435063p004233', 'T202105031441256p023213', 'T202105031451333m023945']\n"
     ]
    }
   ],
   "source": [
    "#decam_transients.loc[decam_transients[\"RA-OBJECT\"]]\n",
    "#decam_transients.loc[decam_transients[\"RA-OBJECT\"] == 211.98278629][\"ObjectID\"].values[0] \n",
    "print(sorted(decam_transients[\"ObjectID\"].values)[-10:])\n",
    "#print(np.isin('T20210530', decam_transients))\n",
    "#decam_transients.loc[decam_transients[\"RA-OBJECT\"] == 211.98278629][\"ObjectID\"].values[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum distance found:  0d03m16.5992s\n",
      "Minimum distance found:  0d03m16.5992s\n",
      "Minimum distance found:  0d03m16.5992s\n",
      "Minimum distance found:  0d03m16.5992s\n",
      "Minimum distance found:  0d03m47.5151s\n",
      "Minimum distance found:  0d03m16.5992s\n",
      "Minimum distance found:  0d03m16.5992s\n",
      "Minimum distance found:  0d03m16.5992s\n",
      "Minimum distance found:  0d03m47.5151s\n",
      "Minimum distance found:  0d05m14.6746s\n",
      "Minimum distance found:  0d08m46.5863s\n",
      "Minimum distance found:  0d08m29.2662s\n",
      "Minimum distance found:  0d10m54.8542s\n",
      "Minimum distance found:  0d06m05.315s\n",
      "Minimum distance found:  0d07m58.4153s\n",
      "Minimum distance found:  0d07m00.7989s\n",
      "Minimum distance found:  0d06m10.268s\n",
      "Minimum distance found:  0d00m48.7596s\n",
      "Minimum distance found:  0d10m17.9326s\n",
      "Minimum distance found:  0d00m48.7596s\n",
      "Minimum distance found:  0d00m48.7596s\n",
      "Minimum distance found:  0d06m10.268s\n",
      "Minimum distance found:  0d11m07.234s\n",
      "Minimum distance found:  0d13m05.3889s\n",
      "Minimum distance found:  0d13m05.7518s\n",
      "Minimum distance found:  0d11m30.2849s\n",
      "Minimum distance found:  0d29m55.2883s\n",
      "Minimum distance found:  0d02m32.0815s\n",
      "Minimum distance found:  0d09m27.3748s\n",
      "Minimum distance found:  0d02m32.0815s\n",
      "Minimum distance found:  0d01m41.5545s\n",
      "Minimum distance found:  0d07m39.4307s\n",
      "Minimum distance found:  0d03m28.3589s\n",
      "Minimum distance found:  0d02m32.0815s\n",
      "Minimum distance found:  0d06m55.3598s\n",
      "Minimum distance found:  0d06m55.3598s\n",
      "Minimum distance found:  0d07m21.2156s\n",
      "Minimum distance found:  0d04m01.3871s\n",
      "Minimum distance found:  0d05m22.4999s\n",
      "Minimum distance found:  0d04m01.3871s\n",
      "Minimum distance found:  0d06m16.0257s\n",
      "Minimum distance found:  0d04m11.3231s\n",
      "Minimum distance found:  0d04m11.3231s\n",
      "Minimum distance found:  0d04m01.3871s\n",
      "Minimum distance found:  0d04m39.7958s\n",
      "Minimum distance found:  0d10m33.5171s\n",
      "Minimum distance found:  0d02m13.3199s\n"
     ]
    }
   ],
   "source": [
    "init_matches_by_date = initial_check(ledger_df = decam_transients, ledger_type = 'DECAM_TAMU')\n",
    "#matches, alert_matches = decam_matching(target_ras = [], target_decs = [], obs_mjd = '', path_in = '', max_sep = 5, sep_units = 'arcsec', ledger_df = [], ledger_type = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 20210101\n",
      "\n",
      " 20210115\n",
      "\n",
      " 20210205\n",
      "\n",
      " 20210208\n",
      "\n",
      " 20210217\n",
      "\n",
      " 20210218\n",
      "\n",
      " 20210221\n",
      "Minimum distance found:  0d00m00.4727s\n",
      "\n",
      " 20210322\n",
      "Minimum distance found:  0d00m00.4727s\n",
      "\n",
      " 20210402\n",
      "\n",
      " 20210405\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.2082s\n",
      "\n",
      " 20210406\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3198s\n",
      "\n",
      " 20210407\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3034s\n",
      "\n",
      " 20210408\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.5481s\n",
      "\n",
      " 20210409\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3075s\n",
      "\n",
      " 20210410\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.2082s\n",
      "\n",
      " 20210411\n",
      "\n",
      " 20210412\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3375s\n",
      "\n",
      " 20210413\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.4077s\n",
      "\n",
      " 20210414\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3358s\n",
      "\n",
      " 20210415\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.4221s\n",
      "\n",
      " 20210416\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.2082s\n",
      "\n",
      " 20210417\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3818s\n",
      "\n",
      " 20210418\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.4412s\n",
      "\n",
      " 20210419\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.0938s\n",
      "\n",
      " 20210420\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3637s\n",
      "\n",
      " 20210428\n",
      "\n",
      " 20210429\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.6694s\n",
      "\n",
      " 20210430\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.0034s\n",
      "\n",
      " 20210501\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3872s\n",
      "\n",
      " 20210502\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.0322s\n",
      "\n",
      " 20210503\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210504\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210505\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210506\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210507\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210508\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210509\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210510\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210511\n",
      "Minimum distance found:  0d00m00.319s\n",
      "\n",
      " 20210512\n",
      "Minimum distance found:  0d00m00.1075s\n",
      "\n",
      " 20210513\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210517\n",
      "Minimum distance found:  0d00m00s\n",
      "\n",
      " 20210518\n",
      "Minimum distance found:  0d00m00.3287s\n",
      "\n",
      " 20210529\n",
      "NaNs found, removing them from array (not FITS) before match.\n",
      "Minimum distance found:  0d00m00.3637s\n",
      "\n",
      " 20210530\n",
      "\n",
      " 20210531\n",
      "Minimum distance found:  0d00m00.319s\n",
      "\n",
      " 20210602\n",
      "Minimum distance found:  0d00m00.3083s\n"
     ]
    }
   ],
   "source": [
    "exclusion_list = [20210101, 20210115, 20210205, 20210208, 20210217, 20210218, 20210402, 20210411, 20210428, 20210530]\n",
    "close_matches = closer_check(init_matches_by_date, ledger_df = decam_transients, ledger_type = 'DECAM_TAMU', exclusion_list = exclusion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print({k:v for k, v in close_matches.items() if v})\n",
    "#tot = 0\n",
    "#for date, val in close_matches.items():\n",
    "    #close_matches[date] = list(set(val))\n",
    "#    tot += len(val)\n",
    "#print(tot)\n",
    "#print(len(close_matches.values()))\n",
    "smushed_YMD = today.iso.split(\" \")[0].replace(\"-\",\"\")\n",
    "write_matches_to_file(\"20201130\", smushed_YMD, close_matches, \"DECAM_TAMU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "336\n"
     ]
    }
   ],
   "source": [
    "file_list = glob.glob(\"./matches/*.txt\")\n",
    "file_dict = read_matches_file(max(file_list, key=os.path.getctime))\n",
    "#print(file_dict)\n",
    "\n",
    "query_template = \"SELECT distinct obsdate, tileid from exposures where expid == \" #obsdate>20210228 \n",
    "\n",
    "#query2 = \"PRAGMA table_info(exposures)\"\n",
    "\n",
    "#cur.execute(query2)\n",
    "#row2 = cur.fetchall()\n",
    "#for i in row2:\n",
    "#    print(i[:])\n",
    "\n",
    "conn = sqlite3.connect(db_filename)\n",
    "conn.row_factory = sqlite3.Row # https://docs.python.org/3/library/sqlite3.html#sqlite3.Row\n",
    "cur = conn.cursor()\n",
    "\n",
    "info_list = []\n",
    "info_list_w_dup = []\n",
    "exp_id_triplets = []\n",
    "\n",
    "for date, v_dict in file_dict.items():\n",
    "    for filename, info in v_dict.items():\n",
    "        #print(v)\n",
    "        exp_id = filename.strip(\"\\t\").split('-')[-1][3:-5] # [:-5] to avoid retained '.fits' at end\n",
    "        row = info[1]\n",
    "        #print(v)\n",
    "        #print(exp_id)\n",
    "        petal_num = filename.split(\"-\")[1][1] # First split \"cframe, [color_band][petal_num], [exp_id].fits\"\n",
    "        query = query_template + exp_id + \";\"\n",
    "\n",
    "        cur.execute(query)\n",
    "        tile_id = cur.fetchone()['tileid']\n",
    "        \n",
    "        #target_id = cur.fetchone()['targetid'] # Grab targetid from cframe file row specified in file\n",
    "\n",
    "        #print(date, tile_id, row)\n",
    "        # See if you can't grab the targetid in modified_cnn_classify while opening zbest using info\n",
    "        # from matches_decam file since we're already accessing zbest\n",
    "        # Check header of zbest files per cframe name/expid/tileid for targetid\n",
    "        # save yourself the trouble ;)\n",
    "\n",
    "        #print('python3 cnn_classify_data.py -d {} -t {} -g'.format(date, tile_id))\n",
    "\n",
    "        #coadd_filename = \"-\".join((\"coadd\", petal_num, str(row_data['tileid']), date)) + \".fits\"\n",
    "        zbest_filename = \"-\".join((\"zbest\", petal_num, str(tile_id), date)) + \".fits\"\n",
    "        #print(tile_id, date, petal_num, row)\n",
    "        info_list_w_dup.append((tile_id, date, petal_num, row))\n",
    "        if (tile_id, petal_num, row) in exp_id_triplets:\n",
    "            pass\n",
    "        else:\n",
    "            info_list.append((tile_id, date, petal_num, row))\n",
    "            exp_id_triplets.append((tile_id, petal_num, row))\n",
    "    \n",
    "    #coadd_filepath = '/'.join((exposure_path, \"daily/tiles\", str(row_data['tileid']), date, coadd_filename)) #coadd-7-81088-20210404.fits\n",
    "    #zbest_filepath = '/'.join((exposure_path, \"daily/tiles\", str(row_data['tileid']), date, zbest_filename)) #zbest-7-81088-20210404.fits\n",
    "    \n",
    "    # Next up - figure out how to feed this to CNN!\n",
    "    #print(coadd_filepath)\n",
    "\n",
    "    #with fits.open(coadd_filepath) as hdu1:\n",
    "        #data_table = Table(hdu1[hdu_num].data) #columns\n",
    "\n",
    "    #targ_id = data_table['TARGETID']\n",
    "    #targ_ra = data_table['TARGET_RA'].data # Now it's a numpy array\n",
    "    #targ_dec = data_table['TARGET_DEC'].data\n",
    "    #targ_mjd = data_table['MJD'][0] some have different versions of this so this is a *bad* idea... at least now I know the try except works!\n",
    "cur.close()\n",
    "print(len(info_list))\n",
    "print(len(info_list_w_dup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_feed.txt', 'w') as f:    \n",
    "    for i in info_list:\n",
    "        f.write(str(i).strip(\"()\").replace(\"'\", \"\"))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes to self - double matches are to be expected, could be worthwhile to compare the spectrum to both\n",
    "# next time setup pipeline to find these individual spectra or at least lists or something to export to other ipynb to then run as a loop\n",
    "# may be tough because... you know... ipynb... we'll see!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESI master",
   "language": "python",
   "name": "desi-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
